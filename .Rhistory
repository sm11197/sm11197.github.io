)
prop.test(
x = c(svy_table["Human Consultancy", "TRUE"], svy_table["Human Debate", "TRUE"]),
n = c((svy_table["Human Consultancy", "TRUE"] + svy_table["Human Consultancy", "FALSE"]), (svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"]))
)
# # Possible table?, high confidence accuracy
# high_conf_data <- subset(judgments_online,
#                          `Final probability correct` <= 0.01 | `Final probability correct` >= 0.99)
# # Create the svytable object for high confidence accuracy
# svy_table_high_conf <- svytable(
#   ~Final_Setting + Final_Accuracy,
#   design = svydesign(
#     ids = ~1,
#     data = subset(high_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
#     weights = ~sampled_consultancies_all_debates_weights_grouped_setting
#   )
# )
# # Call the function for high confidence accuracy
# high_conf_table <- process_table(svy_table_high_conf, round_by = 1)
# high_conf_table
# # Render the high confidence accuracy table
# knitr::kable(high_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# # Possible table?, high confidence accuracy
# low_conf_data <- subset(judgments_online,
#                          `Final probability correct` >= 0.30 & `Final probability correct` <= 0.70)
# # Create the svytable object for high confidence accuracy
# svy_table_low_conf <- svytable(
#   ~Final_Setting + Final_Accuracy,
#   design = svydesign(
#     ids = ~1,
#     data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
#     weights = ~sampled_consultancies_all_debates_weights_grouped_setting
#   )
# )
# Call the function for high confidence accuracy
#low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
#low_conf_table
# Render the high confidence accuracy table
#knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
judgments_online$`Reward penalty 0.5` <- log2(judgments_online$`Final probability correct`) - 0.5*(judgments_online$`Number of judge continues`)
judgments_online$fpc <- judgments_online$`Final probability correct`
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# Test Human Settings only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# TODO: check test for human consultancy & human debate, make table. Might have to rebuild package to get CIs
# Note: see publication in help page for more
# all
pairwise.wilcox.test(judgments_online$`Final probability correct`, judgments_online$Final_Setting)
# human settings
filtered_data <- judgments_online[judgments_online$Final_Setting %in% c("Human Consultancy", "Human Debate"), ]
wilcox.test(
`Final probability correct` ~ Final_Setting,
data = filtered_data,
paired = FALSE,
conf.int = TRUE
)
wilcox.test(
log2(`Final probability correct`) ~ Final_Setting,
data = filtered_data,
paired = FALSE,
conf.int = TRUE
)
# Conduct the Mann-Whitney U test and get the CI
wilcox_test(
formula = `Final probability correct` ~ as.factor(Final_Setting),
data = filtered_data,
#weights = ~sampled_consultancies_all_debates_weights_grouped_setting,
conf.int = TRUE  # Request the confidence interval
)
# The rest is stuff i tried
judgments_online %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpc)) +
labs(y = "fpc", x = "Setting")+
theme_minimal()
judgments_online %>%
group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(Final_Accuracy)) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
labs(y = "acc", x = "Setting")+
theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(fpc))
svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Sean Wang') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
model3 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Sean Wang') + relevel(factor(Final_Setting), 'Human Debate') + `Untimed annotator context`, family = 'binomial', data = judgments_online)
summary(model3)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
debater_turns<- py$debater_turns_agg_simple
debater_turns$check <- paste0(debater_turns$Participant, debater_turns$`Room name`)
paste0(debater_turns$Participant, debater_turns$`Room name`)
sample.rooms <- read.csv("~/Downloads/sample-rooms-2.csv", header=FALSE)
debater_turns <- subset(debater_turns, debater_turns$check %in% sample.rooms_samples)
sort(paste0(sample.rooms$V2, sample.rooms$V1))
sample.rooms <- read.csv("~/Downloads/sample-rooms-2.csv", header=FALSE)
sample.rooms_samples <- sort(paste0(sample.rooms$V2, sample.rooms$V1))
debater_turns$check %in% sample.rooms_sample
debater_turns$check %in% sample.rooms_samples
paste0(debater_turns$Participant, debater_turns$`Room name`)
debater_turns<- py$debater_turns_agg_simple
paste0(debater_turns$Participant, debater_turns$`Room name`)
reticulate::repl_python()
lib_path <- "/Users/bila/git/sm11197.github.io/sm11197.github.io/library"
.libPaths(lib_path) #run first before knitting so it realizes you do have the packages
options(scipen = 999) #prevents scientific notation unless int wide
knitr::opts_chunk$set(class.source = "foldable") #folds code so only results show in HTML
#knitr::opts_chunk$set(cache = TRUE) #so the same output isn't rerun
library(reticulate) #for interop with Python
reticulate::use_virtualenv("/Users/bila/git/for-debate/debate/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted statistics
library(sjstats) #weighted statistics (2)
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(coin) #for rank tests
library(purrr)
library(tidyr)
library(kableExtra)
reticulate::repl_python()
sample <- py$sample
sample <- sample[,c("Room name", "Participant")]
write.csv(sample, "/Users/bila/Downloads/python_sample.csv")
set.seed(123)
# Read in objects from Python with py$
judgments <- py$judgments
judgments_online <- py$judgments_online
correctColor = "#008000"
incorrectColor = "#DC143C"
# Change type into factor so it is read as categories which can be manipulated instead of characters
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)
# Doing some sanity checks
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
#Are the question weights equal for human consultancies?"
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting) ; table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
#What does the accuracy look like for those question weights?
#table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
#table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)
#subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
#table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
#Difference between grouping and not grouping question weights
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights_grouped_setting) ; table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights)
# Balanced consultancies difference between grouping and not grouping question weights
consultancy_condition <- (judgments_online$Sample == TRUE) | (!grepl("Consultancy", judgments_online$Final_Setting))
table(judgments_online[consultancy_condition, ]$Final_Setting, judgments_online[consultancy_condition, ]$sampled_consultancies_all_debates_weights_grouped_setting, judgments_online[consultancy_condition, ]$Final_Accuracy)
table(judgments_online[consultancy_condition, ]$Final_Setting, judgments_online[consultancy_condition, ]$sampled_consultancies_all_debates_weights, judgments_online[consultancy_condition, ]$Final_Accuracy)
# Sampled data (balanced consultancies and sampled debates) difference between grouping and not grouping question weights
table(judgments_online[judgments_online$Sample == TRUE, ]$Final_Setting, judgments_online[judgments_online$Sample == TRUE, ]$sampled_consultancies_debates_weights_grouped_setting)
table(judgments_online[judgments_online$Sample == TRUE, ]$Final_Setting, judgments_online[judgments_online$Sample == TRUE, ]$sampled_consultancies_debates_weights)
# read other sampling
sample.rooms <- read.csv("~/Downloads/sample-rooms-2.csv", header=FALSE)
# Check whether chosen sample in sample.rooms is the same as judgments_online
# based on columns V2 and V1 in sample.rooms and Participant and `Room name` in judgments_online
sample.rooms_samples <- sort(paste0(sample.rooms$V2, sample.rooms$V1))
judgments_online_samples <- paste0(judgments_online[consultancy_condition,]$Participant, judgments_online[consultancy_condition,]$`Room name`)
missing_sample.room <- sample.rooms[sample.rooms_samples %in% judgments_online_samples == FALSE, ]
sampled_judgments_online <- judgments_online[consultancy_condition,]
missing_judgments_online <- sampled_judgments_online[judgments_online_samples %in% sample.rooms_samples == FALSE, ]
judgments_online$check <- paste0(judgments_online$Participant, judgments_online$`Room name`)
matching_sampled_judgments_online <- subset(judgments_online, judgments_online$check %in% sample.rooms_samples)
rooms_hc <- subset(matching_sampled_judgments_online, matching_sampled_judgments_online$Final_Setting == "Human Consultancy")
reticulate::repl_python()
paste("Overall variance is",
var(judgments_online$Final_Accuracy), "(mean way)",
((sum(judgments_online$Final_Accuracy, na.rm = T) / length(judgments_online$Final_Accuracy)) * (1 - (sum(judgments_online$Final_Accuracy, na.rm = T)) / length(judgments_online$Final_Accuracy))) / (length(judgments_online$Final_Accuracy) - 1), "(prop way)")
# Accuracy variation per setting
judgments_online %>%
group_by(Final_Setting) %>%
summarise(
var_mean = var(Final_Accuracy),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
# Accuracy variation per setting (consultancies balanced)
judgments_online[consultancy_condition, ] %>%
group_by(Final_Setting) %>%
summarise(
var_mean = var(Final_Accuracy),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
judgments_online %>%
group_by(base_room_name) %>%
summarise(
var_mean = var(Final_Accuracy),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
judgments_online %>%
group_by(Question) %>%
summarise(
var_mean = var(Final_Accuracy),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
judgments_online[consultancy_condition, ] %>%
group_by(base_room_name) %>%
summarise(
var_mean = var(Final_Accuracy),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
judgments_online[consultancy_condition, ] %>%
group_by(Question) %>%
summarise(
var_mean = var(Final_Accuracy),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
judgments_online[consultancy_condition,] %>%
group_by(base_room_name) %>%
summarise(
var_mean = var(Final_Accuracy, na.rm = T),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy, na.rm = T),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% summarise(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
judgments_online[consultancy_condition,] %>%
group_by(Question) %>%
summarise(
var_mean = var(Final_Accuracy, na.rm = T),
n = length(Final_Accuracy),
x_aka_num_correct = sum(Final_Accuracy, na.rm = T),
p_aka_accuracy = (x_aka_num_correct / n),
var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
) %>% summarise(avg_var_mean = mean(var_mean, na.rm = T),
avg_var_prop = mean(var_prop, na.rm = T))
# Make a function to easily try out different weights
acc_diff_test <- function(design, Setting){
print(design)
freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
print(chisq_result)
pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
print(pairwise_result)
freq_table <- cbind(freq_table, Accuracy = (freq_table[,2] / (freq_table[,1]+freq_table[,2]))*100)
print(freq_table)
}
print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies, NO weights") # still sig
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced # consultancies, question weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights))
print("Balanced consultancies sampled debates, NO weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
print("Now trying manually tests that aren't pairwise + cobfidence intervals for the table")
process_table <- function(svy_table, round_by) {
# Ensure that the input is a svytable object
if (!inherits(svy_table, "svytable")) {
stop("Input must be a svytable object")
}
# Add accuracy
svy_table <- cbind(svy_table, Accuracy = (svy_table[,2] / (svy_table[,1] + svy_table[,2])) * 100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- svy_table[,"Accuracy"] - svy_table["Human Debate", "Accuracy"]
# Bind the difference column to the svy_table
svy_table <- cbind(svy_table, `Difference with Debate` = difference_with_debate)
# Initialize vectors to store confidence interval bounds and p-values
ci_lowers <- c() ; ci_uppers <- c() ; p_values <- c()
# Loop through each setting
for (setting in rownames(svy_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(
x = c(svy_table[setting, "TRUE"], svy_table["Human Debate", "TRUE"]),
n = c((svy_table[setting, "TRUE"] + svy_table[setting, "FALSE"]), (svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"])),
correct = F
)
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- round(results$conf.int[1] * 100,round_by)  # Multiply by 100 to convert to percentage
ci_upper <- round(results$conf.int[2] * 100,round_by)  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
# Change to wanted format (judgments summed, split counts removed)
svy_table <- cbind("n Judgments" = (svy_table[,"FALSE"] + svy_table[,"TRUE"]), svy_table)
svy_table <- svy_table[ , !(colnames(svy_table) %in% c("FALSE", "TRUE"))]
# Concatenate the CI bounds into a single string
ci_strings <- paste0("[", ci_lowers, ", ", ci_uppers, "]")
# Convert svy_table to a data.frame so adding the strings doesn't change the data type for entire matrix
svy_table <- as.data.frame(svy_table)
# Bind the confidence interval bounds and p-values to the svy_table
svy_table <- cbind(svy_table, `95% CI [lower, upper]` = ci_strings, `p val` = p_values)
return(svy_table)
}
# First table, all data accuracy
svy_table_input <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
)
)
svy_table_input_2 <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = matching_sampled_judgments_online,
)
)
# Call the function
final_table <- process_table(svy_table_input, round_by = 3)
final_table
final_table_2 <- process_table(svy_table_input_2, round_by = 3)
final_table_2
knitr::kable(final_table, booktab = TRUE, digits = c(rep(3,3),NA,3))
knitr::kable(final_table_2, booktab = TRUE, digits = c(rep(3,3),NA,3))
svy_table <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
)
)
prop.test(
x = c(svy_table["Human Consultancy", "TRUE"], svy_table["Human Debate", "TRUE"]),
n = c((svy_table["Human Consultancy", "TRUE"] + svy_table["Human Consultancy", "FALSE"]), (svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"]))
)
# # Possible table?, high confidence accuracy
# high_conf_data <- subset(judgments_online,
#                          `Final probability correct` <= 0.01 | `Final probability correct` >= 0.99)
# # Create the svytable object for high confidence accuracy
# svy_table_high_conf <- svytable(
#   ~Final_Setting + Final_Accuracy,
#   design = svydesign(
#     ids = ~1,
#     data = subset(high_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
#     weights = ~sampled_consultancies_all_debates_weights_grouped_setting
#   )
# )
# # Call the function for high confidence accuracy
# high_conf_table <- process_table(svy_table_high_conf, round_by = 1)
# high_conf_table
# # Render the high confidence accuracy table
# knitr::kable(high_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# # Possible table?, high confidence accuracy
# low_conf_data <- subset(judgments_online,
#                          `Final probability correct` >= 0.30 & `Final probability correct` <= 0.70)
# # Create the svytable object for high confidence accuracy
# svy_table_low_conf <- svytable(
#   ~Final_Setting + Final_Accuracy,
#   design = svydesign(
#     ids = ~1,
#     data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
#     weights = ~sampled_consultancies_all_debates_weights_grouped_setting
#   )
# )
# Call the function for high confidence accuracy
#low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
#low_conf_table
# Render the high confidence accuracy table
#knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
judgments_online$`Reward penalty 0.5` <- log2(judgments_online$`Final probability correct`) - 0.5*(judgments_online$`Number of judge continues`)
judgments_online$fpc <- judgments_online$`Final probability correct`
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# Test Human Settings only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# TODO: check test for human consultancy & human debate, make table. Might have to rebuild package to get CIs
# Note: see publication in help page for more
# all
pairwise.wilcox.test(judgments_online$`Final probability correct`, judgments_online$Final_Setting)
# human settings
filtered_data <- judgments_online[judgments_online$Final_Setting %in% c("Human Consultancy", "Human Debate"), ]
wilcox.test(
`Final probability correct` ~ Final_Setting,
data = filtered_data,
paired = FALSE,
conf.int = TRUE
)
wilcox.test(
log2(`Final probability correct`) ~ Final_Setting,
data = filtered_data,
paired = FALSE,
conf.int = TRUE
)
# Conduct the Mann-Whitney U test and get the CI
wilcox_test(
formula = `Final probability correct` ~ as.factor(Final_Setting),
data = filtered_data,
#weights = ~sampled_consultancies_all_debates_weights_grouped_setting,
conf.int = TRUE  # Request the confidence interval
)
# The rest is stuff i tried
judgments_online %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpc)) +
labs(y = "fpc", x = "Setting")+
theme_minimal()
judgments_online %>%
group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(Final_Accuracy)) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
labs(y = "acc", x = "Setting")+
theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(fpc))
svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Sean Wang') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
model3 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Sean Wang') + relevel(factor(Final_Setting), 'Human Debate') + `Untimed annotator context`, family = 'binomial', data = judgments_online)
summary(model3)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
