# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table
#knitr::opts_chunk$set(class.source = "foldable")
options(scipen = 999)
library(tinytex)
library(reticulate)
reticulate::use_virtualenv("/Users/bila/git/for-debate/debate/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted chi
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(purrr) #data manipulation - loops
reticulate::repl_python()
set.seed(123)
judgments <- py$judgments
judgments_online <- py$judgments_online
# Convert the Accuracy column to a factor for better plotting
judgments_online$Final_Accuracy_char <- as.logical.factor(as.character(judgments_online$Final_Accuracy))
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights_grouped_setting)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights)
acc_diff_test <- function(design, Setting){
print(design)
freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
print(freq_table)
chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
print(chisq_result)
pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
print(pairwise_result)
}
print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
acc_diff_test(svydesign(ids = ~1, data = judgments_online, weights = ~sampled_consultancies_debates_weights_grouped_setting))
design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
acc_diff_test(design)
final_table <- svytable(~Final_Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
human_only <- subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting))
human_only$Setting <- droplevels(human_only$Setting)
table(human_only$Setting)
final_table <- svytable(~Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = human_only,
weights = ~sampled_consultancies_all_debates_weights_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
prop_table <- svytable(~Final_Setting+Final_Accuracy, design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)),weights = ~sampled_consultancies_all_debates_weights))
#prop_table <- svytable(~Final_Setting+Final_Accuracy, design = svydesign(ids = ~1, data = subset(judgments_online, !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
prop_table
print(prop.test(c(prop_table["Human Consultancy","TRUE"],prop_table["Human Debate","TRUE"]),c(prop_table["Human Consultancy", "TRUE"]+prop_table["Human Consultancy", "FALSE"], prop_table["Human Debate", "TRUE"]+prop_table["Human Debate", "FALSE"])))
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Aliyaah Toussaint') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
#knitr::opts_chunk$set(class.source = "foldable")
options(scipen = 999)
library(tinytex)
library(reticulate)
reticulate::use_virtualenv("/Users/bila/git/for-debate/debate/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted chi
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(purrr) #data manipulation - loops
reticulate::repl_python()
set.seed(123)
judgments <- py$judgments
judgments_online <- py$judgments_online
# Convert the Accuracy column to a factor for better plotting
judgments_online$Final_Accuracy_char <- as.logical.factor(as.character(judgments_online$Final_Accuracy))
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights_grouped_setting)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights)
acc_diff_test <- function(design, Setting){
print(design)
freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
print(freq_table)
chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
print(chisq_result)
pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
print(pairwise_result)
}
print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
acc_diff_test(svydesign(ids = ~1, data = judgments_online, weights = ~sampled_consultancies_debates_weights_grouped_setting))
design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
acc_diff_test(design)
final_table <- svytable(~Final_Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
human_only <- subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting))
human_only$Setting <- droplevels(human_only$Setting)
table(human_only$Setting)
final_table <- svytable(~Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = human_only,
weights = ~sampled_consultancies_all_debates_weights_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
prop_table <- svytable(~Final_Setting+Final_Accuracy, design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)),weights = ~sampled_consultancies_all_debates_weights))
#prop_table <- svytable(~Final_Setting+Final_Accuracy, design = svydesign(ids = ~1, data = subset(judgments_online, !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
prop_table
print(prop.test(c(prop_table["Human Consultancy","TRUE"],prop_table["Human Debate","TRUE"]),c(prop_table["Human Consultancy", "TRUE"]+prop_table["Human Consultancy", "FALSE"], prop_table["Human Debate", "TRUE"]+prop_table["Human Debate", "FALSE"])))
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Aliyaah Toussaint') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
characters<- py$characters_agg
span_difference_debate_consultancies<-py$filtered_df
ggplot(span_difference_debate_consultancies) +
geom_boxplot(aes(x = `Setting 2`, y = `Span Difference Count`))
ggplot(characters) +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length (unique tokens)")+
theme_minimal()
ggplot(characters) +
geom_boxplot(aes(x = Final_Setting, y = `Quote length`)) +
labs(y = "Total Quote Length (characters)")+
theme_minimal()
pairwise.t.test(characters$quote_length, characters$Final_Setting)
# compute lower and upper whiskers
ylim1 = boxplot.stats(character$quote_length)$stats[c(1, 5)]
# compute lower and upper whiskers
ylim1 = boxplot.stats(characters$quote_length)$stats[c(1, 5)]
ylim1
# scale y limits based on ylim1
p1 = p0 + coord_cartesian(ylim = ylim1*1.05)
tokens <- ggplot(characters) +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length (unique tokens)")+
theme_minimal()
# scale y limits based on ylim1
zoomed_tokens <- tokens + coord_cartesian(ylim = ylim1*1.05)
zoomed_tokens
characters %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(`Quote length`, 0.25),
Q3 = quantile(`Quote length`, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR) %>%
filter(`Quote length` >= lower_bound & `Quote length` <= upper_bound) %>%
select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length (unique tokens)")+
theme_minimal()
characters %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(`Quote length`, 0.25),
Q3 = quantile(`Quote length`, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR) %>%
filter(`Quote length` >= lower_bound & `Quote length` <= upper_bound) %>%
select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length (unique tokens)")+
theme_minimal()
#knitr::opts_chunk$set(class.source = "foldable")
options(scipen = 999)
library(tinytex)
library(reticulate)
reticulate::use_virtualenv("/Users/bila/git/for-debate/debate/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted chi
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(purrr) #data manipulation - loops
reticulate::repl_python()
set.seed(123)
judgments <- py$judgments
judgments_online <- py$judgments_online
# Convert the Accuracy column to a factor for better plotting
judgments_online$Final_Accuracy_char <- as.logical.factor(as.character(judgments_online$Final_Accuracy))
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights_grouped_setting)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights)
acc_diff_test <- function(design, Setting){
print(design)
freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
print(freq_table)
chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
print(chisq_result)
pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
print(pairwise_result)
}
print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
acc_diff_test(svydesign(ids = ~1, data = judgments_online, weights = ~sampled_consultancies_debates_weights_grouped_setting))
design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
acc_diff_test(design)
final_table <- svytable(~Final_Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
human_only <- subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting))
human_only$Setting <- droplevels(human_only$Setting)
table(human_only$Setting)
final_table <- svytable(~Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = human_only,
weights = ~sampled_consultancies_all_debates_weights_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
prop_table <- svytable(~Final_Setting+Final_Accuracy, design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)),weights = ~sampled_consultancies_all_debates_weights))
#prop_table <- svytable(~Final_Setting+Final_Accuracy, design = svydesign(ids = ~1, data = subset(judgments_online, !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
prop_table
print(prop.test(c(prop_table["Human Consultancy","TRUE"],prop_table["Human Debate","TRUE"]),c(prop_table["Human Consultancy", "TRUE"]+prop_table["Human Consultancy", "FALSE"], prop_table["Human Debate", "TRUE"]+prop_table["Human Debate", "FALSE"])))
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Aliyaah Toussaint') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
