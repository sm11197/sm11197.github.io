print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies, NO weights") # still sig
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced # consultancies, question weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights))
print("Balanced consultancies sampled debates, NO weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
print("Now trying manually tests that aren't pairwise + cobfidence intervals for the table")
process_table <- function(svy_table, round_by) {
# Ensure that the input is a svytable object
if (!inherits(svy_table, "svytable")) {
stop("Input must be a svytable object")
}
# Add accuracy
svy_table <- cbind(svy_table, Accuracy = (svy_table[,2] / (svy_table[,1] + svy_table[,2])) * 100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- svy_table[,"Accuracy"] - svy_table["Human Debate", "Accuracy"]
# Bind the difference column to the svy_table
svy_table <- cbind(svy_table, `Difference with Debate` = difference_with_debate)
# Initialize vectors to store confidence interval bounds and p-values
ci_lowers <- c() ; ci_uppers <- c() ; p_values <- c()
# Loop through each setting
for (setting in rownames(svy_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(
x = c(svy_table["Human Debate", "TRUE"], svy_table[setting, "TRUE"]),
n = c((svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"]), (svy_table[setting, "TRUE"] + svy_table[setting, "FALSE"]))
)
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- round(results$conf.int[1] * 100,round_by)  # Multiply by 100 to convert to percentage
ci_upper <- round(results$conf.int[2] * 100,round_by)  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
# Change to wanted format (judgments summed, split counts removed)
svy_table <- cbind("n Judgments (weighted)" = (svy_table[,"FALSE"] + svy_table[,"TRUE"]), svy_table)
svy_table <- svy_table[ , !(colnames(svy_table) %in% c("FALSE", "TRUE"))]
# Concatenate the CI bounds into a single string
ci_strings <- paste0("[", ci_lowers, ", ", ci_uppers, "]")
# Convert svy_table to a data.frame so adding the strings doesn't change the data type for entire matrix
svy_table <- as.data.frame(svy_table)
# Bind the confidence interval bounds and p-values to the svy_table
svy_table <- cbind(svy_table, `95% CI [lower, upper]` = ci_strings, `p val` = p_values)
return(svy_table)
}
# First table, all data accuracy
svy_table_input <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function
final_table <- process_table(svy_table_input, round_by = 1)
final_table
knitr::kable(final_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
high_conf_data <- subset(judgments_online,
`Final probability correct` <= 0.01 | `Final probability correct` >= 0.99)
# Create the svytable object for high confidence accuracy
svy_table_high_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(high_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
high_conf_table <- process_table(svy_table_high_conf, round_by = 1)
high_conf_table
# Render the high confidence accuracy table
knitr::kable(high_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.40 | `Final probability correct` <= 0.60)
# Create the svytable object for high confidence accuracy
svy_table_high_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(high_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
high_conf_table <- process_table(svy_table_high_conf, round_by = 1)
high_conf_table
# Render the high confidence accuracy table
knitr::kable(high_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_high_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.40 | `Final probability correct` <= 0.60)
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.45 | `Final probability correct` <= 0.65)
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.45 | `Final probability correct` <= 0.65)
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.45 | `Final probability correct` <= 0.65)
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.45 | `Final probability correct` < 0.65)
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.45 & `Final probability correct` <= 0.65)
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.40 & `Final probability correct` <= 0.60)
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.30 & `Final probability correct` <= 0.70)
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
judgments_online$fpc <- judgments_online$`Final probability correct`
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# Test Human Settings only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
strat <- py$all_turns
lib_path <- "/Users/bila/git/sm11197.github.io/sm11197.github.io/library"
.libPaths(lib_path) #run first before knitting so it realizes you do have the packages
options(scipen = 999) #prevents scientific notation unless int wide
knitr::opts_chunk$set(class.source = "foldable") #folds code so only results show in HTML
#knitr::opts_chunk$set(cache = TRUE) #so the same output isn't rerun
library(reticulate) #for interop with Python
reticulate::use_virtualenv("/Users/bila/git/sm11197.github.io/sm11197.github.io/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted statistics
library(sjstats) #weighted statistics (2)
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
reticulate::repl_python()
set.seed(123)
# Read in objects from Python with py$
judgments <- py$judgments
judgments_online <- py$judgments_online
# Change type into factor so it is read as categories which can be manipulated instead of characters
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)
# Doing some sanity checks
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
#Are the question weights equal for human consultancies?"
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting) ; table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
#What does the accuracy look like for those question weights?
#table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
#table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)
#subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
#table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
#Difference between grouping and not grouping question weights
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights_grouped_setting) ; table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights)
# Balanced consultancies difference between grouping and not grouping question weights
consultancy_condition <- (judgments_online$Sample == TRUE) | (!grepl("Consultancy", judgments_online$Final_Setting))
table(judgments_online[consultancy_condition, ]$Final_Setting, judgments_online[consultancy_condition, ]$sampled_consultancies_all_debates_weights_grouped_setting, judgments_online[consultancy_condition, ]$Final_Accuracy)
table(judgments_online[consultancy_condition, ]$Final_Setting, judgments_online[consultancy_condition, ]$sampled_consultancies_all_debates_weights, judgments_online[consultancy_condition, ]$Final_Accuracy)
# Sampled data (balanced consultancies and sampled debates) difference between grouping and not grouping question weights
table(judgments_online[judgments_online$Sample == TRUE, ]$Final_Setting, judgments_online[judgments_online$Sample == TRUE, ]$sampled_consultancies_debates_weights_grouped_setting)
table(judgments_online[judgments_online$Sample == TRUE, ]$Final_Setting, judgments_online[judgments_online$Sample == TRUE, ]$sampled_consultancies_debates_weights)
# Make a function to easily try out different weights
acc_diff_test <- function(design, Setting){
print(design)
freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
print(chisq_result)
pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
print(pairwise_result)
freq_table <- cbind(freq_table, Accuracy = (freq_table[,2] / (freq_table[,1]+freq_table[,2]))*100)
print(freq_table)
}
print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies, NO weights") # still sig
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced # consultancies, question weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights))
print("Balanced consultancies sampled debates, NO weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
print("Now trying manually tests that aren't pairwise + cobfidence intervals for the table")
process_table <- function(svy_table, round_by) {
# Ensure that the input is a svytable object
if (!inherits(svy_table, "svytable")) {
stop("Input must be a svytable object")
}
# Add accuracy
svy_table <- cbind(svy_table, Accuracy = (svy_table[,2] / (svy_table[,1] + svy_table[,2])) * 100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- svy_table[,"Accuracy"] - svy_table["Human Debate", "Accuracy"]
# Bind the difference column to the svy_table
svy_table <- cbind(svy_table, `Difference with Debate` = difference_with_debate)
# Initialize vectors to store confidence interval bounds and p-values
ci_lowers <- c() ; ci_uppers <- c() ; p_values <- c()
# Loop through each setting
for (setting in rownames(svy_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(
x = c(svy_table["Human Debate", "TRUE"], svy_table[setting, "TRUE"]),
n = c((svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"]), (svy_table[setting, "TRUE"] + svy_table[setting, "FALSE"]))
)
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- round(results$conf.int[1] * 100,round_by)  # Multiply by 100 to convert to percentage
ci_upper <- round(results$conf.int[2] * 100,round_by)  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
# Change to wanted format (judgments summed, split counts removed)
svy_table <- cbind("n Judgments (weighted)" = (svy_table[,"FALSE"] + svy_table[,"TRUE"]), svy_table)
svy_table <- svy_table[ , !(colnames(svy_table) %in% c("FALSE", "TRUE"))]
# Concatenate the CI bounds into a single string
ci_strings <- paste0("[", ci_lowers, ", ", ci_uppers, "]")
# Convert svy_table to a data.frame so adding the strings doesn't change the data type for entire matrix
svy_table <- as.data.frame(svy_table)
# Bind the confidence interval bounds and p-values to the svy_table
svy_table <- cbind(svy_table, `95% CI [lower, upper]` = ci_strings, `p val` = p_values)
return(svy_table)
}
# First table, all data accuracy
svy_table_input <- svytable(
~Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function
final_table <- process_table(svy_table_input, round_by = 1)
final_table
knitr::kable(final_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
high_conf_data <- subset(judgments_online,
`Final probability correct` <= 0.01 | `Final probability correct` >= 0.99)
# Create the svytable object for high confidence accuracy
svy_table_high_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(high_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
high_conf_table <- process_table(svy_table_high_conf, round_by = 1)
high_conf_table
# Render the high confidence accuracy table
knitr::kable(high_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
# Possible table?, high confidence accuracy
low_conf_data <- subset(judgments_online,
`Final probability correct` >= 0.30 & `Final probability correct` <= 0.70)
# Create the svytable object for high confidence accuracy
svy_table_low_conf <- svytable(
~Final_Setting + Final_Accuracy,
design = svydesign(
ids = ~1,
data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting
)
)
# Call the function for high confidence accuracy
low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
low_conf_table
# Render the high confidence accuracy table
knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))
judgments_online$fpc <- judgments_online$`Final probability correct`
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# Test Human Settings only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# TODO: check test for human consultancy & human debate, make table. Might have to rebuild package to get CIs
# see calibration for confident mistakes
# Note: see publication in help page for more
# The rest is stuff i tried
judgments_online %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpc)) +
labs(y = "fpc", x = "Setting")+
theme_minimal()
judgments_online %>%
group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(Final_Accuracy)) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
labs(y = "acc", x = "Setting")+
theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(fpc))
svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Aliyaah Toussaint') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
debater_turns<- py$debater_turns_agg
span_difference_debate_consultancies<-py$filtered_df
ggplot(span_difference_debate_consultancies) +
geom_boxplot(aes(x = `Setting 2`, y = `Span Difference Count`))
filtered_outliers <- debater_turns %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(quote_length, 0.25),
Q3 = quantile(quote_length, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR)
ggplot(debater_turns) +
geom_boxplot(aes(x = Final_Setting, y = `Quote length`)) +
labs(y = "Total Quote Length (debater_turns)")+
theme_minimal()
filtered <- debater_turns %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(quote_length, 0.25),
Q3 = quantile(quote_length, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR) %>%
filter(quote_length > 0 & quote_length < 750) %>%
select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound)
filtered %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length in a Debate/Consultancy (unique tokens)", x = "Setting")+
theme_minimal()
debater_turns %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length in a Debate/Consultancy (unique tokens)", x = "Setting")+
theme_minimal()
pairwise.t.test(filtered$quote_length, filtered$Final_Setting)
filtered %>% group_by(Final_Setting) %>% summarise(avground = median(quote_length))
debater_turns %>% group_by(Final_Setting) %>% summarise(avground = median(quote_length))
debater_turns <- debater_turns %>%
group_by(`Room name`,) %>%
mutate(`Max judge rounds by room` = max(`Num previous judging rounds`, na.rm = TRUE)) %>%
ungroup()
ggplot(debater_turns) +
geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`)) +
labs(y = 'Max Judging Rounds') +
theme_minimal()
pairwise.t.test(debater_turns$`Max judge rounds by room`, debater_turns$Final_Setting)
filtered <- debater_turns %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(`Max judge rounds by room`, 0.25),
Q3 = quantile(`Max judge rounds by room`, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR) %>%
filter(`Max judge rounds by room` >= lower_bound & `Max judge rounds by room` <= upper_bound) %>%
select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound)
filtered %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`), outlier.shape = NA) +
labs(y = "Rounds", x = "Setting")+
theme_minimal()
debater_turns %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`)) +
labs(y = "Rounds", x = "Setting")+
theme_minimal()
pairwise.t.test(filtered$quote_length, filtered$Final_Setting)
filtered %>% group_by(Final_Setting) %>% summarise(avground = mean(`Max judge rounds by room`))
reticulate::repl_python()
