weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
data(efc)
str(efc)
efc$weight <- abs(rnorm(nrow(efc), 1, .3))
weighted_mannwhitney(c12hour ~ c161sex + weight, efc)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
wilcox.test(efc$c12hour,efc$c161sex)
judgments_online$fpcw <- judgments_online$fpc *judgments_online$sampled_consultancies_all_debates_weights_grouped_setting
wilcox.test(fpcw~Final_Setting, subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), conf.int=T)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
wilcox.test(efc$c12hour,efc$c161sex)
judgments_online$fpc <- judgments_online$`Final probability correct`
judgments_online %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpc)) +
labs(y = "fpc", x = "Setting")+
theme_minimal()
judgments_online %>%
group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(Final_Accuracy)) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
labs(y = "acc", x = "Setting")+
theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(fpc))
svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
judgments_online$fpc <- judgments_online$`Final probability correct`
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("The rest is stuff i tried")
judgments_online %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpc)) +
labs(y = "fpc", x = "Setting")+
theme_minimal()
judgments_online %>%
group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(Final_Accuracy)) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
labs(y = "acc", x = "Setting")+
theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(fpc))
svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# signficant, yey!
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# signficant, yey!
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
# signficant, yey!
wil <- svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# Test Human Setting only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# Test Human Settings only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# Segregate confidently correct and confidently wrong
judgments_online$confidence_label <- case_when(
judgments_online$`Final probability correct` > 0.95 ~ "Confidently Correct",
judgments_online$`Final probability correct` < 0.05 ~ "Confidently Wrong",
TRUE ~ "Neutral"
)
# Filter out only the rows with confidently correct and confidently wrong labels
filtered_data <- judgments_online %>%
filter(confidence_label != "Neutral")
# Count the occurrences for each setting and confidence label
count_data <- filtered_data %>%
group_by(`Final_Setting`, confidence_label) %>%
summarise(count = n())
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = confidence_label)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Confident Mistakes and Correct by Setting", y = "Count", x = "Setting") +
theme_minimal()
library(ggplot2)
library(dplyr)
correctColor = "green"
incorrectColor = "crimson"
# Segregate confidently correct and confidently wrong
judgments_online$confidence_label <- case_when(
judgments_online$`Final probability correct` > 0.95 ~ "Confidently Correct",
judgments_online$`Final probability correct` < 0.05 ~ "Confidently Wrong",
TRUE ~ "Neutral"
)
# Filter out only the rows with confidently correct and confidently wrong labels
filtered_data <- judgments_online %>%
filter(confidence_label != "Neutral")
# Count the occurrences for each setting and confidence label
count_data <- filtered_data %>%
group_by(`Final_Setting`, confidence_label) %>%
summarise(count = n())
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = confidence_label)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_manual(values = c("Confidently Correct" = correctColor, "Confidently Wrong" = incorrectColor)) +
labs(title = "Confident Mistakes and Correct by Setting", y = "Count", x = "Setting") +
theme_minimal()
library(ggplot2)
library(dplyr)
correctColor = "#008000"
incorrectColor = "#DC143C"
# Segregate confidently correct and confidently wrong
judgments_online$confidence_label <- case_when(
judgments_online$`Final probability correct` > 0.95 ~ "Confidently Correct",
judgments_online$`Final probability correct` < 0.05 ~ "Confidently Wrong",
TRUE ~ "Neutral"
)
# Filter out only the rows with confidently correct and confidently wrong labels
filtered_data <- judgments_online %>%
filter(confidence_label != "Neutral")
# Count the occurrences for each setting and confidence label
count_data <- filtered_data %>%
group_by(`Final_Setting`, confidence_label) %>%
summarise(count = n())
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = confidence_label)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_manual(values = c("Confidently Correct" = correctColor, "Confident
library(ggplot2)
library(dplyr)
correctColor = "#008000"
incorrectColor = "#DC143C"
# Segregate confidently correct and confidently wrong
judgments_online$confidence_label <- case_when(
judgments_online$`Final probability correct` > 0.95 ~ "Confidently Correct",
judgments_online$`Final probability correct` < 0.05 ~ "Confidently Wrong",
TRUE ~ "Neutral"
)
# Filter out only the rows with confidently correct and confidently wrong labels
filtered_data <- judgments_online %>%
filter(confidence_label != "Neutral")
# Count the occurrences for each setting and confidence label
count_data <- filtered_data %>%
group_by(`Final_Setting`, confidence_label) %>%
summarise(count = n())
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = confidence_label)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_manual(values = c("Confidently Correct" = correctColor, "Confidently Wrong" = incorrectColor)) +
labs(title = "Confident Mistakes and Correct by Setting", y = "Count", x = "Setting") +
theme_minimal()
# Calculate the color value for each row
judgments_online$color_value <- log2(judgments_online$`Final probability correct`) - (0.05 * judgments_online$`Number of judge continues`)
# Count the occurrences for each setting and 'Final probability correct' value
count_data <- judgments_online %>%
group_by(`Final_Setting`, `Final probability correct`, color_value) %>%
summarise(count = n())
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = color_value, group = `Final probability correct`)) +
geom_bar(stat = "identity", position = "stack") +
scale_fill_gradient(low = "blue", high = "red") +  # Adjust as needed
labs(title = "Distribution of Final Probabilities by Setting", y = "Count", x = "Setting") +
theme_minimal()
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = color_value, group = `Final probability correct`)) +
geom_bar(stat = "identity", position = "stack") +
scale_fill_gradient(low = "#DC143C", high = "#008000") +  # Adjust as needed
labs(title = "Distribution of Final Probabilities by Setting", y = "Count", x = "Setting") +
theme_minimal()
# Plot
# Count the occurrences for each setting and 'Final probability correct' value
count _data <- judgments_online %>%
# Plot
# Count the occurrences for each setting and 'Final probability correct' value
count_data <- judgments_online %>%
group_by(`Final_Setting`, `Final probability correct`, color_value) %>%
summarise(count = n()) %>%
mutate(percentage = count / sum(count) * 100)  # Calculate percentage
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = percentage, fill = color_value, group = `Final probability correct`)) +
geom_bar(stat = "identity", position = "stack") +
scale_fill_gradient(low = "#DC143C", high = "#008000") +  # Adjust as needed
labs(title = "Distribution of Final Probabilities by Setting", y = "Percentage", x = "Setting") +
theme_minimal()
# Plot
# Count the occurrences for each setting and 'Final probability correct' value
count_data <- judgments_online %>%
group_by(`Final_Setting`, `Final probability correct`, color_value) %>%
summarise(count = n()) %>%
mutate(percentage = count / sum(count) * 100)  # Calculate percentage
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = percentage, fill = color_value, group = `Final probability correct`)) +
geom_bar(stat = "identity", position = "stack") +
scale_fill_gradient(low = "#DC143C", high = "#008000") +  # Adjust as needed
labs(title = "Distribution of Final Probabilities by Setting", y = "Percentage", x = "Setting") +
theme_minimal()
# Plot
# Count the occurrences for each setting and 'Final probability correct' value
count_data <- judgments_online %>%
group_by(`Final_Setting`, `Final probability correct`, color_value) %>%
summarise(count = n()) %>%
mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = percentage, fill = color_value, group = `Final probability correct`)) +
geom_bar(stat = "identity", position = "stack") +
scale_fill_gradient(low = "#DC143C", high = "#008000") +  # Adjust as needed
labs(title = "Distribution of Final Probabilities by Setting", y = "Percentage", x = "Setting") +
theme_minimal()
# Count the occurrences for each setting and 'Final probability correct' value
count_data <- judgments_online %>%
group_by(`Final_Setting`, `Final probability correct`, color_value) %>%
summarise(count = n())
# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = color_value, group = `Final probability correct`)) +
geom_bar(stat = "identity", position = "stack") +
scale_fill_gradient(low = "#DC143C", high = "#008000") +  # Adjust as needed
labs(title = "Distribution of Final Probabilities by Setting", y = "Count", x = "Setting") +
theme_minimal()
knitr::opts_chunk$set(class.source = "foldable")
lib_path <- "/Users/bila/git/sm11197.github.io/sm11197.github.io/library"
.libPaths(lib_path)
options(scipen = 999)
library(reticulate) #for interop with Python
reticulate::use_virtualenv("/Users/bila/git/sm11197.github.io/sm11197.github.io/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted statistics
library(sjstats) #weighted statistics (2)
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(purrr) #data manipulation - efficient loops
knitr::opts_chunk$set(class.source = "foldable")
lib_path <- "/Users/bila/git/sm11197.github.io/sm11197.github.io/library"
.libPaths(lib_path)
options(scipen = 999)
library(reticulate) #for interop with Python
reticulate::use_virtualenv("/Users/bila/git/sm11197.github.io/sm11197.github.io/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted statistics
library(sjstats) #weighted statistics (2)
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(purrr) #data manipulation - efficient loops
reticulate::repl_python()
set.seed(123)
judgments <- py$judgments
judgments_online <- py$judgments_online
# Convert the Accuracy column to a factor for better plotting
judgments_online$Final_Accuracy_char <- as.logical.factor(as.character(judgments_online$Final_Accuracy))
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting)
table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights_grouped_setting)
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_debates_weights)
# Make a function to easily try out different weights
acc_diff_test <- function(design, Setting){
print(design)
freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
print(chisq_result)
pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
print(pairwise_result)
freq_table <- cbind(freq_table, Accuracy = (freq_table[,2] / (freq_table[,1]+freq_table[,2]))*100)
print(freq_table)
}
print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
acc_diff_test(svydesign(ids = ~1, data = judgments_online, weights = ~sampled_consultancies_debates_weights_grouped_setting))
design = svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
acc_diff_test(design)
print("Now trying manually tests that aren't pairwise + cobfidence intervals for the table")
## To maybe do: refactor this into function?
final_table <- svytable(~Final_Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
print("Second table, human settings only")
human_only <- subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting))
human_only$Setting <- droplevels(human_only$Setting)
table(human_only$Setting)
final_table <- svytable(~Setting+Final_Accuracy,
design = svydesign(ids = ~1,
data = human_only,
weights = ~sampled_consultancies_all_debates_weights_setting))
final_table
# Add accuracy
final_table <- cbind(final_table, Accuracy = (final_table[,2] / (final_table[,1]+final_table[,2]))*100)
# Calculate the difference in accuracy for each row compared to "Human Debate"
difference_with_debate <- final_table[,"Accuracy"] - final_table["Human Debate", "Accuracy"]
# Bind the difference column to the final_table
final_table <- cbind(final_table, difference_with_debate)
# Loop through each setting
ci_lowers <- c()
ci_uppers <- c()
p_values <- c()
# Loop through each setting
for (setting in rownames(final_table)) {
# Use prop.test to compare the setting's accuracy with "Human Debate"
results <- prop.test(c(final_table["Human Debate", "TRUE"], final_table[setting, "TRUE"]), c((final_table["Human Debate", "TRUE"]+final_table["Human Debate", "FALSE"]), (final_table[setting, "TRUE"]+final_table[setting, "FALSE"])))
# Extract the confidence interval and store it as a string in the format "lower - upper"
ci_lower <- results$conf.int[1] * 100  # Multiply by 100 to convert to percentage
ci_upper <- results$conf.int[2] * 100  # Multiply by 100 to convert to percentage
ci_lowers <- c(ci_lowers, ci_lower)
ci_uppers <- c(ci_uppers, ci_upper)
p_values <- c(p_values, results$p.value)
}
final_table <- cbind(final_table, ci_lowers, ci_uppers, p_values)
final_table
# Display the updated table using knitr::kable
knitr::kable(final_table, booktab = TRUE,  digits = c(rep(1,6),3),
col.names = c("# Incorrect (weighted)", "# Correct (weighted)", "Accuracy", "Difference", "95% CI Lower Limit","95% CI Upper Limit","p-value"))
judgments_online$fpc <- judgments_online$`Final probability correct`
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# Test Human Settings only
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "wilcoxon")
svyranktest(fpc~Final_Setting,
svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
test = "median")
# TODO: check test for human consultancy & human debate, make table. Might have to rebuild package to get CIs
# see calibration for confident mistakes
# Note: see publication in help page for more
# The rest is stuff i tried
judgments_online %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpc)) +
labs(y = "fpc", x = "Setting")+
theme_minimal()
judgments_online %>%
group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(Final_Accuracy)) %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
labs(y = "acc", x = "Setting")+
theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)
svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
fpcmean = mean(fpc))
svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model1)
table(model1$fitted.values > 0.5)
table(judgments_online$Final_Accuracy)
model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Aliyaah Toussaint') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)
summary(model2)
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting),
data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
reticulate::repl_python()
characters<- py$characters_agg
span_difference_debate_consultancies<-py$filtered_df
ggplot(span_difference_debate_consultancies) +
geom_boxplot(aes(x = `Setting 2`, y = `Span Difference Count`))
filtered_outliers <- characters %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(quote_length, 0.25),
Q3 = quantile(quote_length, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR)
ggplot(characters) +
geom_boxplot(aes(x = Final_Setting, y = `Quote length`)) +
labs(y = "Total Quote Length (characters)")+
theme_minimal()
filtered <- characters %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(quote_length, 0.25),
Q3 = quantile(quote_length, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR) %>%
filter(quote_length > 0 & quote_length < 750) %>%
select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound)
filtered %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length in a Debate/Consultancy (unique tokens)", x = "Setting")+
theme_minimal()
characters %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
labs(y = "Total Quote Length in a Debate/Consultancy (unique tokens)", x = "Setting")+
theme_minimal()
pairwise.t.test(filtered$quote_length, filtered$Final_Setting)
filtered %>% group_by(Final_Setting) %>% summarise(avground = median(quote_length))
characters %>% group_by(Final_Setting) %>% summarise(avground = median(quote_length))
characters <- characters %>%
group_by(`Room name`,) %>%
mutate(`Max judge rounds by room` = max(`Num previous judging rounds`, na.rm = TRUE)) %>%
ungroup()
ggplot(characters) +
geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`)) +
labs(y = 'Max Judging Rounds') +
theme_minimal()
pairwise.t.test(characters$`Max judge rounds by room`, characters$Final_Setting)
filtered <- characters %>%
group_by(Final_Setting) %>%
mutate(Q1 = quantile(`Max judge rounds by room`, 0.25),
Q3 = quantile(`Max judge rounds by room`, 0.75),
IQR = Q3 - Q1,
lower_bound = Q1 - 1.5 * IQR,
upper_bound = Q3 + 1.5 * IQR) %>%
filter(`Max judge rounds by room` >= lower_bound & `Max judge rounds by room` <= upper_bound) %>%
select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound)
filtered %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`), outlier.shape = NA) +
labs(y = "Rounds", x = "Setting")+
theme_minimal()
characters %>%
ggplot() +
geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`)) +
labs(y = "Rounds", x = "Setting")+
theme_minimal()
pairwise.t.test(filtered$quote_length, filtered$Final_Setting)
filtered %>% group_by(Final_Setting) %>% summarise(avground = mean(`Max judge rounds by room`))
reticulate::repl_python()
