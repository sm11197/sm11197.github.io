---
title: 'Results'
output:
  pdf_document:
    keep_tex: True
---
>Notes:  
- Some of this is already in or was based on the blogpost/interface code. Hit show to see code. I switch between R and Python 
- Some of this won't make it to the paper. You can probably skip preprocessing unless you want to check certain things, example: did we make sure to remove judgments based on X condition



```{r setup, include=FALSE}
lib_path <- "/Users/bila/git/sm11197.github.io/sm11197.github.io/library"
.libPaths(lib_path) #run first before knitting so it realizes you do have the packages

options(scipen = 999) #prevents scientific notation unless int wide
knitr::opts_chunk$set(class.source = "foldable") #folds code so only results show in HTML
#knitr::opts_chunk$set(cache = TRUE) #so the same output isn't rerun

library(reticulate) #for interop with Python
reticulate::use_virtualenv("/Users/bila/git/for-debate/debate/.venv")
library(ggplot2) #graphs
library(dplyr) #data manipulation
library(survey) #weighted statistics
library(sjstats) #weighted statistics (2)
library(lme4) #linear mixed models
library(lmerTest) #tests for the above
library(brms) #bayesian regression models
library(boot) #bootstrap resampling for confidence intervals
library(coin) #for rank tests

library(purrr)
library(tidyr)
library(kableExtra)
```

# Preprocessing

## Importing, filtering, and adding columns

We have 3 sets of data from the interface:
```{python preprocessing 1}
import pandas as pd
import numpy as np
import altair as alt
import math as math
import matplotlib.pyplot as plt
import re
pd.options.mode.chained_assignment = None  # default='warn'

# Load summaries that can be downloaded from the interface
data_path = "/Users/bila/git/for-debate/debate/save/official/summaries/"
debates = pd.read_csv(data_path + "debates.csv", keep_default_na=True)
sessions = pd.read_csv(data_path + "sessions.csv", keep_default_na=True)
turns = pd.read_csv(data_path + "turns.csv", keep_default_na=True)
print(f' {debates.shape} - Debates') ;
print(f'{sessions.shape} - Sessions, which has multiple rows (of participants) for each debate') ;
print(f'{turns.shape} - and Turns, which has multiple rows (of participant turns) for each debate')

# Only include debates within a given period
debates["Start time"] = pd.to_datetime(debates["Start time"], unit="ms")
debates["End time"] = pd.to_datetime(debates["End time"], unit="ms")
debates["Last modified time"] = pd.to_datetime(debates["Last modified time"], unit="ms")
debates = debates[
    (debates["Start time"] > pd.to_datetime("10/02/23", format="%d/%m/%y")) &
    (debates["End time"] < pd.to_datetime("01/09/23", format="%d/%m/%y"))
]
### for filtering to when we had AI debates: 16/07/23
# Filter sessions & turns to only the selected debates
sessions = sessions.merge(debates[["Room name"]], how="inner", on="Room name")
turns = turns.merge(debates[["Room name"]], how="inner", on="Room name")
print(f'We have {len(debates)} debates when filtering out the initial pilots last fall')

# Secondary analysis: Question Difficulty
# Create new columns with bin labels
debates['Untimed annotator context bins'] = pd.cut(debates['Untimed annotator context'].round(), bins=[0, 1, 2, 3, 4], labels=['1', '2', '3', '4'], right=True)
#print(debates['Untimed annotator context'].round().value_counts()) #check
#print(debates['Untimed annotator context bins'].value_counts()) #check
debates['Speed annotator accuracy bins'] = pd.cut(debates['Speed annotator accuracy'].round(1), bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5,0.6], labels=['0', '0.1', '0.2','0.3','0.4','0.5'], right=False)
## respectively, those speed annotator accuracies probably mean 0 right, 1 right, 2 right
#print(debates['Speed annotator accuracy'].round(1).value_counts().sort_index()) #check #0.5 acc? 
#print(debates['Speed annotator accuracy bins'].value_counts().sort_index()) #check

debates['Final_Accuracy'] = debates['Final probability correct'] > 0.5

print(f'Average accuracy per context required by question:\n{debates.groupby("Untimed annotator context bins")["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size")}\nOverall accuracy goes down the more context is required')

print(f'Average accuracy per difficulty based on speed annotator accuracy:\n{debates.groupby("Speed annotator accuracy bins")["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size")}\nHm, this seems less likely to be a good indicator of question difficulty')


# Determine settings for each row
def setups(row):
    if 'GPT-4' in (row['Honest debater'], row['Dishonest debater']):
        if row['Is single debater']:
            return "AI Consultancy " + ("Honest" if row['Has honest debater'] else "Dishonest")
        else:
            return "AI Debate"
    else:
        if row['Is single debater']:
            return "Human Consultancy " + ("Honest" if row['Has honest debater'] else "Dishonest")
        else:
            return "Human Debate"

debates['Setting'] = debates.apply(setups, axis=1)
# Agregate settings - the 4 that we normally talk about:
debates['Final_Setting'] = debates['Setting'].str.replace(' Honest', '').str.replace(' Dishonest', '')
```

## Merging, filtering for judgments
```{python preprocessing 2}
# Merge sessions with debates, so we have each judge's final probability correct and the debate's metadata
source = sessions.merge(
        debates[["Room name", "Debater A","Debater B","Honest debater", "Dishonest debater",
                 "Is single debater", 'Has honest debater',
                 "Final_Setting", "Setting",
                 "Question", "Article ID", "Story length",
                 "Speed annotator accuracy bins","Untimed annotator context bins",
                 "Speed annotator accuracy","Untimed annotator context", "Is offline",
                 'End time', 'Last modified time']],
        how="left",
        on="Room name",
    )
print(f'After merging debates with sessions, we have the following participant counts for those debates:\n{source["Role"].value_counts()}') 
#[source['Is over'] == True] to check for completed online/offline debates

# Filter out incomplete judgments
judgments = source[source['Final probability correct'].notnull()]
print(f'After filtering to judges that have finalized their judgment, we have the following judgments per role:\n{judgments["Role"].value_counts()}\nfor a total of {len(judgments)} judgments.')

print(f'Of those judgments, we have this much for each setting (not consolidating honest - dishonest consultancies):\n{judgments["Setting"].value_counts()}')

judgments['Final_Accuracy'] = judgments['Final probability correct'] > 0.5

print(f'Of those judgments, we have this much for each setting (aggregated):\n{judgments.groupby("Final_Setting")["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size").sort_index()}')

# Remove judges who see the story more than once
judgments['base_room_name'] = judgments['Room name'].str.extract('(.*)\d+$', expand=False).fillna(judgments['Room name'])
judgments = judgments.sort_values(by=['base_room_name','End time']).groupby(['Participant', 'base_room_name']).first().reset_index()

print(f'1. We then filter to judgments where the judge has only seen a story once, and now we have this much for each setting (aggregated):\n{judgments.groupby("Final_Setting")["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size").sort_index()}')


# Filter to online judges only
judgments_online = judgments[judgments["Role"] == "Judge"]
print(f'2. We\'ll make a copy of the online judgments only leaving us with the following judgments:\n{judgments_online.groupby("Final_Setting")["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size")}') #halves the data


judgments_online = judgments_online[judgments_online['Untimed annotator context bins'].isin(['2', '3', '4'])]

print(f'3. We then filter to judgments which require more than a sentence or two, and now we have this much for each setting (aggregated):\n{judgments_online.groupby(["Final_Setting"])["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size")}\nThis is where debate accuracy drops')

pd.set_option('display.max_columns', None)
total_counts_for_setting = judgments_online.groupby('Final_Setting').size()
result = judgments_online.groupby(["Final_Setting", "Untimed annotator context bins"], observed=False).agg(
    Proportion_True=pd.NamedAgg(column='Final_Accuracy', aggfunc=lambda x: x.mean()),
    Count=pd.NamedAgg(column='Final_Accuracy', aggfunc='size'),
    Proportion_Count=pd.NamedAgg(column='Final_Setting', aggfunc=lambda x: len(x) / total_counts_for_setting[x.mode()])
)
print(f'Are the difficult questions equally enough distributed amongst settings?:\n{result}')
pd.reset_option('display.max_columns')
```
So question difficulty isn't perfectly balanced... but consultancies have a different relationship with question difficulty anyway?
**need a second opinion**
We might at least want to ratio it better for AI settings...

## Trying to balance the data

1.  Balancing honest & dishonest consultancies
2.  Question weights

### Balancing honest & dishonest consultancies
```{python balance consultancies}
def balance_consultancies(df, sample_setting, random_state):
    """
    Sample distinct questions, then use common questions, ensure equal counts.
    """
    consult_df = df[df['Setting'].str.contains(sample_setting, na=False)]
    honest_df = consult_df[consult_df['Setting'].str.contains('Honest')]
    dishonest_df = consult_df[consult_df['Setting'].str.contains('Dishonest')]
    sample_column_name = f'{sample_setting} Sample'
    df[sample_column_name] = False
    # Separate into distinct and common questions
    # First, let's extract the combinations of 'Article ID' and 'Question' for both honest and dishonest dataframes
    honest_combinations = set(honest_df[['Article ID', 'Question']].itertuples(index=False, name=None))
    dishonest_combinations = set(dishonest_df[['Article ID', 'Question']].itertuples(index=False, name=None))
    # Identifying the common and distinct combinations
    common_combinations = honest_combinations.intersection(dishonest_combinations)
    distinct_honest_combinations = honest_combinations - common_combinations
    distinct_dishonest_combinations = dishonest_combinations - common_combinations
    # Filtering the original dataframes based on these combinations to get distinct and common dataframes
    common_honest_df = honest_df[honest_df.set_index(['Article ID', 'Question']).index.isin(common_combinations)]
    common_dishonest_df = dishonest_df[dishonest_df.set_index(['Article ID', 'Question']).index.isin(common_combinations)]
    distinct_honest_df = honest_df[honest_df.set_index(['Article ID', 'Question']).index.isin(distinct_honest_combinations)]
    distinct_dishonest_df = dishonest_df[dishonest_df.set_index(['Article ID', 'Question']).index.isin(distinct_dishonest_combinations)]
    def extract_correct_index(sample_df):
        if isinstance(sample_df.index, pd.MultiIndex):
            return sample_df.index.get_level_values(2)
        else:
            return sample_df.index
    # Get distinct consultancies
    sample_size = min(len(distinct_honest_df), len(distinct_dishonest_df))
    honest_sample = distinct_honest_df.sample(sample_size, random_state=random_state)
    dishonest_sample = distinct_dishonest_df.sample(sample_size, random_state=random_state)
    df.loc[extract_correct_index(honest_sample), sample_column_name] = True
    df.loc[extract_correct_index(dishonest_sample), sample_column_name] = True
    # Drop sampled questions from distinct dataframes
    honest_remove_distinct = set(honest_sample[['Article ID', 'Question']].itertuples(index=False, name=None))
    dishonest_remove_distinct = set(dishonest_sample[['Article ID', 'Question']].itertuples(index=False, name=None))
    distinct_honest_df = distinct_honest_df[~distinct_honest_df.index.isin(honest_sample.index)]
    distinct_dishonest_df = distinct_dishonest_df[~distinct_dishonest_df.index.isin(dishonest_sample.index)]
    honest_distinct_remaining = len(distinct_honest_df)
    dishonest_distinct_remaining = len(distinct_dishonest_df)
    # Sample from remaining distinct questions, using common questions for the other (bigger count) setting as needed
    if honest_distinct_remaining > dishonest_distinct_remaining:
        sample_size = min(honest_distinct_remaining, len(common_dishonest_df))
        honest_sample = distinct_honest_df.sample(sample_size, random_state=random_state)
        dishonest_sample = common_dishonest_df.sample(sample_size, random_state=random_state)
        df.loc[extract_correct_index(dishonest_sample), sample_column_name] = True
        df.loc[extract_correct_index(honest_sample), sample_column_name] = True
        dishonest_remove_common = set(dishonest_sample[['Article ID', 'Question']].itertuples(index=False, name=None))
        common_dishonest_df = common_dishonest_df[~common_dishonest_df.index.isin(dishonest_sample.index)]
        common_honest_df = common_honest_df[~common_honest_df.index.isin(honest_sample.index)]
    else:
        sample_size = min(dishonest_distinct_remaining, len(common_honest_df))
        honest_sample = common_honest_df.sample(sample_size, random_state=random_state)
        dishonest_sample = distinct_dishonest_df.sample(sample_size, random_state=random_state)
        df.loc[extract_correct_index(dishonest_sample), sample_column_name] = True
        df.loc[extract_correct_index(honest_sample), sample_column_name] = True
        honest_remove_common = set(honest_sample[['Article ID', 'Question']].itertuples(index=False, name=None))
        common_dishonest_df = common_dishonest_df[~common_dishonest_df.index.isin(dishonest_sample.index)]
        common_honest_df = common_honest_df[~common_honest_df.index.isin(honest_sample.index)]
    # Remaining independent samples from common_honest_df
    if len(common_honest_df) > 0 or len(common_dishonest_df) > 0:
        sample_size = min(len(common_honest_df), len(common_dishonest_df))
        honest_sample = common_honest_df.sample(sample_size, random_state=random_state)
        dishonest_sample = common_dishonest_df.sample(sample_size, random_state=random_state)
        df.loc[extract_correct_index(honest_sample), sample_column_name] = True
        df.loc[extract_correct_index(dishonest_sample), sample_column_name] = True
    return df
  
# Run the sampling to balance the consultancies
judgments_online = balance_consultancies(judgments_online, 'Human Consultancy', random_state = 12345)
judgments_online = balance_consultancies(judgments_online, 'AI Consultancy', random_state = 12345)
# Create one sample column for easier indexing, create mask
#sample_columns = [col for col in judgments_online.columns if 'Sample' in col]
#judgments_online['Sample'] = judgments_online[sample_columns].any(axis=1)
#consultancy_balanced = (~judgments_online['Setting'].str.contains('Consultancy', case=False, na=False)) | (judgments_online['Sample'] == True)

#print(f'Accuracy after balancing consultancies:\n{judgments_online[consultancy_balanced].groupby(["Final_Setting"])["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size")}')


#from statsmodels.stats.proportion import proportions_ztest

#def run_experiment(judgments_online):
#    judgments_online['Sample'] = False
#    judgments_online = balance_consultancies(judgments_online, 'Human Consultancy')
#    judgments_online = balance_consultancies(judgments_online, 'AI Consultancy')
#    sample_columns = [col for col in judgments_online.columns if 'Sample' in col]
#    judgments_online['Sample'] = judgments_online[sample_columns].any(axis=1)
#    consultancy_balanced = (~judgments_online['Setting'].str.contains('Consultancy', case=False, na=False)) | (judgments_online['Sample'] == True)
#    result = judgments_online[consultancy_balanced].groupby(["Final_Setting"])["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(), Total_Count="size")
#    return result

# Number of iterations
#num_iterations = 1000

# Store results from each iteration
#results = []
#p_vals = []
# Run the experiment multiple times
#for _ in range(num_iterations):
#    result = run_experiment(judgments_online.copy())  # Use a copy to ensure original data remains unchanged
#    results.append(result)
#    # Run the proportions test
#    group_human_debate = result.loc['Human Debate']
#    group_human_consultancy = result.loc['Human Consultancy']
#    count = [group_human_debate.Proportion_True * group_human_debate.Total_Count, group_human_consultancy.Proportion_True * group_human_consultancy.Total_Count]
#    nobs = [group_human_debate.Total_Count, group_human_consultancy.Total_Count]
#    z_stat, p_val = proportions_ztest(count, nobs)
#    p_vals.append(p_val)

# Calculate the average of the results
#average_result = pd.concat(results).groupby(level=0).mean()

#print(f'\nAverage accuracy after {num_iterations} iterations:\n{average_result}')

#print(f'pval mean: {np.mean(p_vals)}')
```

### Balance debates? (not actually used)
```{python balancing debates}
def balance_debates(df, sample_setting, random_state):
    debates_df = df[df['Setting'].str.contains(sample_setting, na=False)]
    sample_column_name = f'{sample_setting} Sample'
    df[sample_column_name] = False
    def extract_correct_index(sample_df):
        if isinstance(sample_df.index, pd.MultiIndex):
            return sample_df.index.get_level_values(2)
        else:
            return sample_df.index
    # Get distinct consultancies
    sample_size = len(debates_df.groupby(['Question', 'Article ID']))
    sample_debates = debates_df.groupby(['Question', 'Article ID']).apply(lambda x: x.sample(1, random_state=random_state)).sample(sample_size, random_state=random_state)
    df.loc[extract_correct_index(sample_debates), sample_column_name] = True
    return df

# Run the sampling to balance the consultancies
judgments_online = balance_debates(judgments_online, 'Human Debate', random_state = 123)
judgments_online = balance_debates(judgments_online, 'AI Debate', random_state = 123)

```

### Question weights

```{python question weights}
# Create one sample column for easier indexing, create mask
sample_columns = [col for col in judgments_online.columns if 'Sample' in col]
consultancy_sample_columns = [col for col in judgments_online.columns if 'Consultancy Sample' in col]
judgments_online['Sample'] = judgments_online[sample_columns].any(axis=1)
judgments_online['Consultancy Sample'] = judgments_online[consultancy_sample_columns].any(axis=1)
consultancy_balanced = (~judgments_online['Setting'].str.contains('Consultancy', case=False, na=False)) | (judgments_online['Consultancy Sample'] == True)

print(f'Accuracy per setting (aggregated) after balancing:\n{judgments_online[consultancy_balanced].groupby("Final_Setting")["Final_Accuracy"].agg(Proportion_True=lambda x: x.mean(),Total_Count="size")}\nAccuracies remain pretty similar')


def question_weights(data, columns, weight_column_name, consultancy_sample=None, debate_sample=None):
    # 0. Make a copy of the original data for weight calculations
    working_data = data.copy()
    # 0.1. Custom filtering based on the 'Setting' column
    consultancy_condition = working_data['Setting'].str.contains('Consultancy', case=False, na=False)
    debate_condition = ~consultancy_condition
    if consultancy_sample is not None:
        consultancy_condition &= (working_data['Sample'] == consultancy_sample)
    if debate_sample is not None: # uncomment if we want to sample debates
        debate_condition &= (working_data['Sample'] == debate_sample)
    combined_mask = consultancy_condition | debate_condition
    working_data = working_data[combined_mask]
    # 1. Calculate the frequency of each question in the dataset
    question_frequency = working_data.groupby(columns).size()
    # 2. Invert the frequency to get the weight for each question
    question_weights = 1 / question_frequency
    # 3. Normalize the weights
    #question_weights = question_weights / question_weights.sum() * len(question_weights)
    # 4. Assign the calculated weights to the original data and fill missing values with 0
    data.loc[combined_mask, weight_column_name] = data[combined_mask].set_index(columns).index.map(question_weights).fillna(0).values
    data[weight_column_name].fillna(0, inplace=True)
    return data

judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question'], 
    weight_column_name='initial_question_weights'
)
judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question', 'Final_Setting'], 
    weight_column_name='initial_question_weights_grouped_setting'
)

```


```{python final question weights}
def print_weight_summary_by_setting(df, weight_column, consultancy_sample=None):
    consultancy_condition = df['Setting'].str.contains('Consultancy', case=False, na=False)
    if consultancy_sample is not None:
        consultancy_condition &= (df['Consultancy Sample'] == consultancy_sample)
    for setting in sorted(df['Setting'].unique()):
        total_weight = df[df['Setting'] == setting][weight_column].sum()
        print(f"Total {weight_column} for {setting}: {total_weight:.2f}")
    print("\n")

print('Unsampled consultancies/debates (initial) weights, by group setting')
print_weight_summary_by_setting(judgments_online, 'initial_question_weights_grouped_setting')

# Recalculate weights for balanced consultancies, all debates
judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question'], 
    weight_column_name='sampled_consultancies_all_debates_weights',
    consultancy_sample=True
)
judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question', 'Setting'], 
    weight_column_name='sampled_consultancies_all_debates_weights_setting',
    consultancy_sample=True
)
judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question', 'Final_Setting'], 
    weight_column_name='sampled_consultancies_all_debates_weights_grouped_setting',
    consultancy_sample=True
)
print('Consultancy balanced weights, not grouped - (not balanced, would have to change balancing function...')
print_weight_summary_by_setting(judgments_online[consultancy_balanced], 'sampled_consultancies_all_debates_weights', consultancy_sample=True)
print('Consultancy balanced weights, grouped by Setting - see that the consultancies are balanced between those with honest or dishonest consultant')
print_weight_summary_by_setting(judgments_online[consultancy_balanced], 'sampled_consultancies_all_debates_weights_setting', consultancy_sample=True)
print('Consultancy balanced weights, grouped by Final Setting')
print_weight_summary_by_setting(judgments_online[consultancy_balanced], 'sampled_consultancies_all_debates_weights_grouped_setting', consultancy_sample=True)


judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question'], 
    weight_column_name='sampled_consultancies_debates_weights',
    consultancy_sample=True,
    debate_sample=True
)
judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question', 'Setting'], 
    weight_column_name='sampled_consultancies_debates_weights_setting',
    consultancy_sample=True,
    debate_sample=True
)
judgments_online = question_weights(
    data=judgments_online, 
    columns=['Article ID', 'Question', 'Final_Setting'], 
    weight_column_name='sampled_consultancies_debates_weights_grouped_setting',
    consultancy_sample=True,
    debate_sample=True
)
```

Note: we are not balancing between settings(?), 
and more counts of the human debate settings are on the same questions..?

## Judge Accuracy Vis

```{python judge_accuracy_setting, out.width = "100%", dpi = 300}
import altair
# set graphic parameters
correctColor = "green"
incorrectColor = "crimson"
nullColor = "lightgrey"
onlineColor = "orange"
offlineColor = "blue"
aggColor = "black"
fullWidth = 1400
# fullHeight = 600
def outcomes_by_field(source, rowEncoding = None):
    source['outcome'] = source.apply(
        lambda row: "incomplete" if math.isnan(row['Final probability correct'])
        else "tie" if row['Final probability correct'] == 0.5
        else "correct" if row['Final probability correct'] > 0.5
        else "incorrect",
        axis=1
    )
    source['Final probability correct (with imputation)'] = source.apply(
        lambda row: 0.5 if math.isnan(row['Final probability correct'])
        else row['Final probability correct'],
        axis=1
    )
    source['Final probability correct (dist from half)'] = source.apply(
        lambda row: 0.0 if math.isnan(row['Final probability correct'])
        else abs(row['Final probability correct'] - 0.5),
        axis=1
    )
    if rowEncoding is None:
        groups = ['outcome']
    else:
        groups = ['outcome', rowEncoding.field]
    base = alt.Chart(
        source
    ).transform_joinaggregate(
        groupby=groups,
        group_count='count()'
    ).encode(
        y=alt.Y('outcome:N', scale=alt.Scale(domain=['correct', 'incorrect', 'tie', 'incomplete']))
    )
    if rowEncoding is not None:
        base = base.encode(row=rowEncoding)
    main_bar = base.mark_bar().encode(
        x=alt.X('count():Q', axis=None),
        color = alt.Color(
            'Final probability correct (with imputation):Q',
            scale=alt.Scale(range=[incorrectColor, nullColor, correctColor], domain=[0.0, 1.0]),
            title='Final Probability\nAssigned to\nCorrect Answer'
        ),
        order=alt.Order(
            'Final probability correct (dist from half):Q',
            sort='ascending'
        ),
        tooltip = [
            'outcome:N',
            alt.Tooltip('group_count:Q', title="Judgments"),
            alt.Tooltip('count():Q', title = 'Judgments with this probability'),
            'Final probability correct:Q'
        ]
    ).properties(width=fullWidth)# height=fullHeight/3)
    return main_bar

def accuracy_by_field(source, by_turn: bool = False, yEncoding = None, invert = False):
    if by_turn:
        prob_correct_field = 'Probability correct'
    else:
        prob_correct_field = 'Final probability correct'
    if source.get('Final probability assigned') is not None:
        prob_assigned_field = 'Final probability assigned'
    else:
        prob_assigned_field = prob_correct_field
    if yEncoding is None:
        groups = []
    else:
        groups = [yEncoding.field]
    base = alt.Chart(source).transform_joinaggregate(
        total = "count()",
        groupby = groups
    ).transform_calculate(
        proportion = '1 / datum.total'
    ).transform_calculate(
        is_correct = f'datum["{prob_correct_field}"] > 0.5 ? 1 : 0',
        is_win = f'datum["{prob_assigned_field}"] > 0.5 ? 1 : 0',
        is_not_correct = f'datum["{prob_correct_field}"] <= 0.5 ? 1 : 0'
    )
    if yEncoding is not None:
        base = base.encode(y=yEncoding)
    main_bar = base.mark_bar().encode(
        x=alt.X('sum(proportion):Q',
            axis=alt.Axis(title=None, format='.0%', labelExpr="(datum.value * 5) % 1 ? null : datum.label"),
            scale=alt.Scale(domain=[0.0, 1.0])
        ),
        color=alt.Color(f'{prob_correct_field}:Q', scale=alt.Scale(range=[incorrectColor, nullColor, correctColor], domain=[0.0, 1.0]), title = ["Probability","Assigned"], legend=alt.Legend(format=".0%", labelFontSize=12, titleFontSize=12, gradientLength=225, gradientThickness=35)),
        order=alt.Order(
            f'{prob_assigned_field}:Q',
            sort='descending' if not invert else 'ascending'
        ),
        tooltip = [
            'count():Q',
            'total:Q',
            'sum(proportion):Q',
            f'{prob_correct_field}:Q',
            'Room name:N',
            'Participant:N'
        ]
    ).properties(width=fullWidth)# height=fullHeight/12)
    prop_color = aggColor
    # rule_thickness = 1.0
    # err_thickness = 1.0
    point_size = 25.0
    mean_field = 'is_win' if not invert else 'is_not_correct'
    gold_err = (base
    ).mark_rule(
        # extent='ci',
        color=prop_color,
    ).encode(
        x=f'ci0({mean_field}):Q',
        x2=f'ci1({mean_field}):Q',
        # scale=alt.Scale(zero=False)
        tooltip=[]
    )
    gold_mean = base.mark_point(
        # thickness=2.0
        color=prop_color, size=point_size, filled=True
    ).encode(
        x=alt.X(f'mean({mean_field}):Q',
            scale=alt.Scale(zero=False)),
    )
    gold_mean_num = base.mark_text(
        color=prop_color,
        align='left',
        baseline='bottom',
        fontSize=24,
        fontWeight='bold',
        dx=4,
        dy=-4
    ).encode(
        text=alt.Text(f'mean({mean_field}):Q', format='.0%'),
        x=alt.X(f'mean({mean_field}):Q',
            scale=alt.Scale(zero=False)),
    )
    return main_bar + gold_err + gold_mean + gold_mean_num

def accuracy_by_judge_setting(setting,data_frame_source):
    source = data_frame_source
    yEncoding = alt.Y(field = setting, type='nominal', title=None)
    outcomes_source = source
    accuracy_source = source[source['Final probability correct'].notna()]
    chart = alt.vconcat(
        accuracy_by_field(
            accuracy_source,
            yEncoding = yEncoding
        ).properties(title=alt.TitleParams(text="Judge Accuracy", fontSize=28)),
    ).resolve_scale(x = 'independent')
    return chart.configure(
            padding = {"left": 7, "top": 5, "right": 5, "bottom": 5},
            axis = alt.Axis(labelFontSize=20,labelLimit=300),
            legend = alt.LegendConfig(disable = True)
            ).configure_view(
        step=65,  # adjust the step parameter for margins
        )

accuracy_by_judge_setting(setting = 'Final_Setting', data_frame_source = judgments_online.loc[
    (judgments_online['Consultancy Sample'] == True) |
    (~judgments_online['Final_Setting'].str.contains("Consultancy", na=False))
])


#chart.save('judge_accuracy_settings.png', scale_factor=4)
```

```{python judge_accuracy_setting_consultancies, out.width = "100%", dpi = 300}
consultancies = judgments_online.loc[judgments_online['Consultancy Sample'] == True]
consultancies['Setting'] = consultancies['Setting'].apply(lambda x: ' '.join(x.split()[:-1]) + f" ({x.split()[-1].lower()})")
accuracy_by_judge_setting(setting = 'Setting', data_frame_source = consultancies)

sample = judgments_online.loc[
    (judgments_online['Consultancy Sample'] == True) |
    (~judgments_online['Final_Setting'].str.contains("Consultancy", na=False))
]
```
## Load into R environment

```{r R pre}
sample <- py$sample
sample <- sample[,c("Room name", "Participant")]
write.csv(sample, "/Users/bila/Downloads/python_sample.csv")
set.seed(123)
# Read in objects from Python with py$
judgments <- py$judgments
judgments_online <- py$judgments_online
correctColor = "#008000"
incorrectColor = "#DC143C"
# Change type into factor so it is read as categories which can be manipulated instead of characters
judgments_online$Participant <- as.factor(judgments_online$Participant)
judgments_online$Setting <- as.factor(judgments_online$Setting)

# Doing some sanity checks
subset_dishonest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Dishonest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
subset_honest <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Setting == 'Human Consultancy Honest', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
#Are the question weights equal for human consultancies?"
table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting) ; table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting)
#What does the accuracy look like for those question weights?
#table(subset_dishonest$sampled_consultancies_all_debates_weights_grouped_setting, subset_dishonest$Final_Accuracy)
#table(subset_honest$sampled_consultancies_all_debates_weights_grouped_setting, subset_honest$Final_Accuracy)



#subset_human_consultancies <- judgments_online[judgments_online$`Human Consultancy Sample` == TRUE & judgments_online$Final_Setting == 'Human Consultancy', c("sampled_consultancies_all_debates_weights_grouped_setting","Final_Accuracy")]
#table(subset_human_consultancies$sampled_consultancies_all_debates_weights_grouped_setting, subset_human_consultancies$Final_Accuracy)
#Difference between grouping and not grouping question weights
table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights_grouped_setting) ; table(judgments_online$Final_Setting, judgments_online$sampled_consultancies_all_debates_weights)


# Balanced consultancies difference between grouping and not grouping question weights
consultancy_condition <- (judgments_online$Sample == TRUE) | (!grepl("Consultancy", judgments_online$Final_Setting))
table(judgments_online[consultancy_condition, ]$Final_Setting, judgments_online[consultancy_condition, ]$sampled_consultancies_all_debates_weights_grouped_setting, judgments_online[consultancy_condition, ]$Final_Accuracy)
table(judgments_online[consultancy_condition, ]$Final_Setting, judgments_online[consultancy_condition, ]$sampled_consultancies_all_debates_weights, judgments_online[consultancy_condition, ]$Final_Accuracy)


# Sampled data (balanced consultancies and sampled debates) difference between grouping and not grouping question weights
table(judgments_online[judgments_online$Sample == TRUE, ]$Final_Setting, judgments_online[judgments_online$Sample == TRUE, ]$sampled_consultancies_debates_weights_grouped_setting)
table(judgments_online[judgments_online$Sample == TRUE, ]$Final_Setting, judgments_online[judgments_online$Sample == TRUE, ]$sampled_consultancies_debates_weights)
```
## Robustness Checks
```{r match_samples}
# read other sampling
sample.rooms <- read.csv("~/Downloads/sample-rooms-2.csv", header=FALSE)
# Check whether chosen sample in sample.rooms is the same as judgments_online
# based on columns V2 and V1 in sample.rooms and Participant and `Room name` in judgments_online
sample.rooms_samples <- sort(paste0(sample.rooms$V2, sample.rooms$V1))
judgments_online_samples <- paste0(judgments_online[consultancy_condition,]$Participant, judgments_online[consultancy_condition,]$`Room name`)

missing_sample.room <- sample.rooms[sample.rooms_samples %in% judgments_online_samples == FALSE, ]
sampled_judgments_online <- judgments_online[consultancy_condition,]
missing_judgments_online <- sampled_judgments_online[judgments_online_samples %in% sample.rooms_samples == FALSE, ]

judgments_online$check <- paste0(judgments_online$Participant, judgments_online$`Room name`)
matching_sampled_judgments_online <- subset(judgments_online, judgments_online$check %in% sample.rooms_samples)
rooms_hc <- subset(matching_sampled_judgments_online, matching_sampled_judgments_online$Final_Setting == "Human Consultancy")
```

```{python}
different_sample = r.rooms_hc 
different_sample.groupby(['Question', 'Article ID']).size().value_counts().sum()
judgments_online[(judgments_online['Setting'].str.contains('Human Consultancy')) & (judgments_online['Consultancy Sample'] == True)].groupby(['Question', 'Article ID']).size().value_counts().sum()


filtered_df1 = different_sample.groupby(['Question', 'Article ID']).filter(lambda x: len(x) > 2)
filtered_df2 = different_sample.groupby(['Question', 'Article ID']).filter(lambda x: len(x) <= 2)

filtered_df1["Untimed annotator context bins"].value_counts()
filtered_df2["Untimed annotator context bins"].value_counts()

filtered_df1["Final_Accuracy"].mean()
filtered_df2["Final_Accuracy"].mean()

judgments_online[judgments_online['Final_Setting']=="Human Debate"].groupby(['Question', 'Article ID']).size().value_counts()
judgments_online[judgments_online['Final_Setting']=="AI Debate"].groupby(['Question', 'Article ID']).size().value_counts()
```


```{r variance check}
paste("Overall variance is", 
      var(judgments_online$Final_Accuracy), "(mean way)",
      ((sum(judgments_online$Final_Accuracy, na.rm = T) / length(judgments_online$Final_Accuracy)) * (1 - (sum(judgments_online$Final_Accuracy, na.rm = T)) / length(judgments_online$Final_Accuracy))) / (length(judgments_online$Final_Accuracy) - 1), "(prop way)")
# Accuracy variation per setting
judgments_online %>%
  group_by(Final_Setting) %>%
  summarise(
    var_mean = var(Final_Accuracy),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))
# Accuracy variation per setting (consultancies balanced)
judgments_online[consultancy_condition, ] %>%
  group_by(Final_Setting) %>%
  summarise(
    var_mean = var(Final_Accuracy),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))

judgments_online %>%
  group_by(base_room_name) %>%
  summarise(
    var_mean = var(Final_Accuracy),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))
judgments_online %>%
  group_by(Question) %>%
  summarise(
    var_mean = var(Final_Accuracy),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))



judgments_online[consultancy_condition, ] %>%
  group_by(base_room_name) %>%
  summarise(
    var_mean = var(Final_Accuracy),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))
judgments_online[consultancy_condition, ] %>%
  group_by(Question) %>%
  summarise(
    var_mean = var(Final_Accuracy),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% mutate(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))


judgments_online[consultancy_condition,] %>%
  group_by(base_room_name) %>%
  summarise(
    var_mean = var(Final_Accuracy, na.rm = T),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy, na.rm = T),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% summarise(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))
judgments_online[consultancy_condition,] %>%
  group_by(Question) %>%
  summarise(
    var_mean = var(Final_Accuracy, na.rm = T),
    n = length(Final_Accuracy),
    x_aka_num_correct = sum(Final_Accuracy, na.rm = T),
    p_aka_accuracy = (x_aka_num_correct / n),
    var_prop = (p_aka_accuracy * (1 - p_aka_accuracy)) / (n - 1)
  ) %>% summarise(avg_var_mean = mean(var_mean, na.rm = T),
               avg_var_prop = mean(var_prop, na.rm = T))
```

# Results


## Difference in Accuracy

```{r weighted chi}
# Make a function to easily try out different weights
acc_diff_test <- function(design, Setting){
  print(design)
  freq_table <- svytable(~Final_Setting+Final_Accuracy, design)
  chisq_result <- svychisq(~Final_Setting+Final_Accuracy, design, statistic = "Chisq")
  print(chisq_result)
  pairwise_result <- pairwise.prop.test(freq_table, p.adjust.method="none", alternative="two.sided")
  print(pairwise_result)
  freq_table <- cbind(freq_table, Accuracy = (freq_table[,2] / (freq_table[,1]+freq_table[,2]))*100)
  print(freq_table)
}

print("Really raw")
acc_diff_test(svydesign(ids = ~1, data = judgments))
print("Raw")
acc_diff_test(svydesign(ids = ~1, data = judgments_online))
print("Balanced consultancies, NO weights") # still sig
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting))))
print("Balanced consultancies, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
print("Balanced # consultancies, question weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights))
print("Balanced consultancies sampled debates, NO weights")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
print("Balanced consultancies sampled debates, question weights (grouped settings)")
acc_diff_test(svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE)))
svytable(~Final_Setting+Final_Accuracy, svydesign(ids = ~1, data = subset(judgments_online, `Sample` == TRUE), weights = ~sampled_consultancies_debates_weights_grouped_setting))

print("Now trying manually tests that aren't pairwise + cobfidence intervals for the table")


process_table <- function(svy_table, round_by) {
  # Ensure that the input is a svytable object
  if (!inherits(svy_table, "svytable")) {
    stop("Input must be a svytable object")
  }
  # Add accuracy
  svy_table <- cbind(svy_table, Accuracy = (svy_table[,2] / (svy_table[,1] + svy_table[,2])) * 100)
  # Calculate the difference in accuracy for each row compared to "Human Debate"
  difference_with_debate <- svy_table[,"Accuracy"] - svy_table["Human Debate", "Accuracy"]
  # Bind the difference column to the svy_table
  svy_table <- cbind(svy_table, `Difference with Debate` = difference_with_debate)
  # Initialize vectors to store confidence interval bounds and p-values
  ci_lowers <- c() ; ci_uppers <- c() ; p_values <- c()
  # Loop through each setting
  for (setting in rownames(svy_table)) {
    # Use prop.test to compare the setting's accuracy with "Human Debate"
    results <- prop.test(
      x = c(svy_table[setting, "TRUE"], svy_table["Human Debate", "TRUE"]),
      n = c((svy_table[setting, "TRUE"] + svy_table[setting, "FALSE"]), (svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"])),
      correct = F
    )
    # Extract the confidence interval and store it as a string in the format "lower - upper"
    ci_lower <- round(results$conf.int[1] * 100,round_by)  # Multiply by 100 to convert to percentage
    ci_upper <- round(results$conf.int[2] * 100,round_by)  # Multiply by 100 to convert to percentage
    ci_lowers <- c(ci_lowers, ci_lower)
    ci_uppers <- c(ci_uppers, ci_upper)
    p_values <- c(p_values, results$p.value)
  }
  # Change to wanted format (judgments summed, split counts removed)
  svy_table <- cbind("n Judgments" = (svy_table[,"FALSE"] + svy_table[,"TRUE"]), svy_table)
  svy_table <- svy_table[ , !(colnames(svy_table) %in% c("FALSE", "TRUE"))]
  # Concatenate the CI bounds into a single string
  ci_strings <- paste0("[", ci_lowers, ", ", ci_uppers, "]")
  # Convert svy_table to a data.frame so adding the strings doesn't change the data type for entire matrix
  svy_table <- as.data.frame(svy_table)
  # Bind the confidence interval bounds and p-values to the svy_table
  svy_table <- cbind(svy_table, `95% CI [lower, upper]` = ci_strings, `p val` = p_values)
  return(svy_table)
}

# First table, all data accuracy
svy_table_input <- svytable(
  ~Final_Setting + Final_Accuracy, 
  design = svydesign(
    ids = ~1, 
    data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
  )
)

svy_table_input_2 <- svytable(
  ~Final_Setting + Final_Accuracy, 
  design = svydesign(
    ids = ~1, 
    data = matching_sampled_judgments_online,
  )
)

# Call the function
final_table <- process_table(svy_table_input, round_by = 3)
final_table

final_table_2 <- process_table(svy_table_input_2, round_by = 3)
final_table_2

knitr::kable(final_table, booktab = TRUE, digits = c(rep(3,3),NA,3))
knitr::kable(final_table_2, booktab = TRUE, digits = c(rep(3,3),NA,3))


svy_table <- svytable(
  ~Final_Setting + Final_Accuracy, 
  design = svydesign(
    ids = ~1, 
    data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
  )
)
prop.test(
      x = c(svy_table["Human Consultancy", "TRUE"], svy_table["Human Debate", "TRUE"]),
      n = c((svy_table["Human Consultancy", "TRUE"] + svy_table["Human Consultancy", "FALSE"]), (svy_table["Human Debate", "TRUE"] + svy_table["Human Debate", "FALSE"]))
    )


# # Possible table?, high confidence accuracy
# high_conf_data <- subset(judgments_online, 
#                          `Final probability correct` <= 0.01 | `Final probability correct` >= 0.99)

# # Create the svytable object for high confidence accuracy
# svy_table_high_conf <- svytable(
#   ~Final_Setting + Final_Accuracy, 
#   design = svydesign(
#     ids = ~1, 
#     data = subset(high_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
#     weights = ~sampled_consultancies_all_debates_weights_grouped_setting
#   )
# )

# # Call the function for high confidence accuracy
# high_conf_table <- process_table(svy_table_high_conf, round_by = 1)
# high_conf_table

# # Render the high confidence accuracy table
# knitr::kable(high_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))






# # Possible table?, high confidence accuracy
# low_conf_data <- subset(judgments_online, 
#                          `Final probability correct` >= 0.30 & `Final probability correct` <= 0.70)

# # Create the svytable object for high confidence accuracy
# svy_table_low_conf <- svytable(
#   ~Final_Setting + Final_Accuracy, 
#   design = svydesign(
#     ids = ~1, 
#     data = subset(low_conf_data, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)),
#     weights = ~sampled_consultancies_all_debates_weights_grouped_setting
#   )
# )

# Call the function for high confidence accuracy
#low_conf_table <- process_table(svy_table_low_conf, round_by = 1)
#low_conf_table

# Render the high confidence accuracy table
#knitr::kable(low_conf_table, booktab = TRUE, digits = c(rep(1,3),NA,3))

```
## Difference in final probability correct

```{r final probability correct}
judgments_online$`Reward penalty 0.5` <- log2(judgments_online$`Final probability correct`) - 0.5*(judgments_online$`Number of judge continues`)
judgments_online$fpc <- judgments_online$`Final probability correct`
  
# Weighted Kruskal-Wallis
svyranktest(fpc~Final_Setting, svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting))
# Test Human Settings only
svyranktest(fpc~Final_Setting, 
            svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
            test = "wilcoxon")
svyranktest(fpc~Final_Setting, 
            svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting),
            test = "median")
# TODO: check test for human consultancy & human debate, make table. Might have to rebuild package to get CIs
# Note: see publication in help page for more

# all
pairwise.wilcox.test(judgments_online$`Final probability correct`, judgments_online$Final_Setting)
# human settings
filtered_data <- judgments_online[judgments_online$Final_Setting %in% c("Human Consultancy", "Human Debate"), ]
wilcox.test(
  `Final probability correct` ~ Final_Setting, 
  data = filtered_data,
  paired = FALSE,
  conf.int = TRUE
)
wilcox.test(
  log2(`Final probability correct`) ~ Final_Setting, 
  data = filtered_data,
  paired = FALSE,
  conf.int = TRUE
)
# Conduct the Mann-Whitney U test and get the CI
wilcox_test(
  formula = `Final probability correct` ~ as.factor(Final_Setting), 
  data = filtered_data,
  #weights = ~sampled_consultancies_all_debates_weights_grouped_setting,
  conf.int = TRUE  # Request the confidence interval
)



# The rest is stuff i tried
judgments_online %>%
  ggplot() +
  geom_boxplot(aes(x = Final_Setting, y = fpc)) +
  labs(y = "fpc", x = "Setting")+
  theme_minimal()
judgments_online %>%
  group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
                                                           fpcmean = mean(Final_Accuracy)) %>%
  ggplot() +
  geom_boxplot(aes(x = Final_Setting, y = fpcmean)) +
  labs(y = "acc", x = "Setting")+
  theme_minimal()
consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)



human_consultancy_design <- svydesign(ids = ~1, data = subset(judgments_online, `Human Consultancy Sample` == TRUE | !grepl("Consultancy", Final_Setting) & !grepl("AI", Final_Setting)), weights = ~sampled_consultancies_all_debates_weights_grouped_setting)


svyranktest(fpc~Final_Setting, human_consultancy_design)
judgments_online %>% group_by(Final_Setting) %>% summarise(fpcmed = median(fpc),
                                                           fpcmean = mean(fpc))

svyranktest(fpc~Final_Setting, consultancy_design, test = "median")
svyranktest(fpc~Final_Setting, consultancy_design, test = "wilcoxon")
svyranktest(fpc~Final_Setting, consultancy_design, test = "vanderWaerden")
weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)



weighted_mannwhitney(fpc ~ Final_Setting + sampled_consultancies_all_debates_weights_grouped_setting, judgments_online)
```

## Models

### Logistic regression
```{r}
#judgments_online$Final_Setting <- relevel(judgments_online$Final_Setting, ref = "Human Debate")
model1 <- glm(Final_Accuracy ~ relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)

summary(model1)
table(model1$fitted.values > 0.5) 
table(judgments_online$Final_Accuracy)

model2 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Sean Wang') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments_online)

summary(model2)

model3 <- glm(Final_Accuracy ~ relevel(factor(Participant),'Sean Wang') + relevel(factor(Final_Setting), 'Human Debate') + `Untimed annotator context`, family = 'binomial', data = judgments_online)

summary(model3)
```

### LMER

```{r lmer}
random.intercept.model = lmer(`Final probability correct` ~ (1|Final_Setting), 
                              data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)


random.intercept.model = lmer(`Final probability correct` ~ (1|Participant) + (1|Final_Setting), 
                              data = judgments, REML = TRUE)
judgments$random.intercept.preds = predict(random.intercept.model)
summary(random.intercept.model)
ranef(random.intercept.model)
ranova(random.intercept.model)

```

### BRMS

```{r brms, out.width = "100%", dpi = 300}
#brm1 <- brm(data = judgments_online,
#             formula = as.numeric(Final_Accuracy) | trials(2) ~ 1 + (1 | Final_Setting),
#             family = binomial("identity"),
#             iter = 2000, warmup = 1000, chains = 4, cores = 4,
#             control = list(adapt_delta = .975, max_treedepth = 20),
#             seed = 190831)
#plot(brm1)
```

## Efficiency

### Quotes %, caveats

```{python quote_length}
debater_turns = turns.merge(
        judgments_online[["Room name", "Question", "Story length",
                 "Untimed annotator context","Untimed annotator context bins",
                 "Setting", "Final_Setting", "Final_Accuracy",
                 "Is offline","Number of judge continues","Participant"]],
        how="inner",
        on="Room name",
    )



# Filtering for specific roles
debater_turns = debater_turns[debater_turns['Role (honest/dishonest)'].isin(['Honest debater', 'Dishonest debater'])]

# Aggregating function to concatenate quote spans
def custom_join(series):
    return ' '.join(filter(lambda x: isinstance(x, str), series))


aggregates = {
    'Quote length': 'sum',
    'Story length': 'mean',
    'Number of judge continues': 'max',
    'Participant quote span': custom_join
}
debater_turns_agg = debater_turns.groupby('Room name').agg(aggregates).reset_index()
debater_turns_agg_simple = debater_turns_agg.merge(
    debater_turns[['Room name', 'Setting', 'Final_Setting', 'Question', 'Untimed annotator context bins','Final_Accuracy', 'Participant_y']].drop_duplicates(),
    on='Room name'
)


# Extracting the spans
def extract_spans(span_str):
    """Extract numerical spans from the given string."""
    if pd.isna(span_str):
        return []
    spans = re.findall(r'<<(\d+)-(\d+)>>', span_str)
    return [(int(start), int(end)) for start, end in spans]

# Functions to compute and compare spans across settings
def extract_numbers_from_span(span_str):
    spans = extract_spans(span_str)
    numbers = set()
    for start, end in spans:
        numbers.update(range(int(start), int(end)+1))
    return numbers
def quote_length(span_str):
  spans = extract_spans(span_str)
  numbers = set()
  for start, end in spans:
    numbers.update(range(int(start), int(end)))
  return numbers

# Merging overlapping spans
def merge_overlapping_spans(span_str):
    if not isinstance(span_str, str):
        return span_str
    spans = extract_spans(span_str)
    if not spans:
        return span_str
    spans.sort(key=lambda x: x[0])
    merged = [spans[0]]
    for current in spans:
        previous = merged[-1]
        if current[0] <= previous[1]:
            upper_bound = max(previous[1], current[1])
            merged[-1] = (previous[0], upper_bound)
        else:
            merged.append(current)
    return ' '.join(f'<<{start}-{end}>>' for start, end in merged)


debater_turns_agg_simple["quote_length"] = debater_turns_agg_simple["Participant quote span"].apply(lambda row: len(quote_length(row)))




# Identify questions with more than one setting and filter out the debater_turns dataframe
questions_with_multi_settings = debater_turns.groupby("Question").filter(lambda x: len(x["Setting"].unique()) > 1)["Question"].unique()
debater_turns_filtered = debater_turns[debater_turns["Question"].isin(questions_with_multi_settings)]

# Aggregating data
aggregates = {
    'Quote length': 'sum',
    'Story length': 'mean',
    'Number of judge continues': 'max',
    'Participant quote span': custom_join
}
# Grouping by 'Room name' and aggregating
debater_turns_filtered_by_room = debater_turns_filtered.groupby('Room name').agg(aggregates).reset_index()

# Merging the aggregated results with the original data to reintroduce the desired columns
debater_turns_agg = debater_turns_filtered_by_room.merge(
    debater_turns_filtered[['Room name', 'Setting', 'Final_Setting', 'Question', 'Untimed annotator context bins','Final_Accuracy']].drop_duplicates(),
    on='Room name'
)

debater_turns_agg["quote_length"] = debater_turns_agg["Participant quote span"].apply(lambda row: len(quote_length(row)))

# Merge overlapping spans after the aggregation
debater_turns_agg["merged_quote_spans"] = debater_turns_agg["Participant quote span"].apply(merge_overlapping_spans)

#debater_turns_agg["merged_quote_length"] = debater_turns_agg["Participant quote span"].apply(lambda row: len(quote_length(row)))
#print(debater_turns_agg["merged_quote_length"][1])
#print((debater_turns_agg["merged_quote_length"]==debater_turns_agg["quote_length"]).value_counts())

#print((debater_turns_agg['quote_length'].fillna(0)/debater_turns_agg['Story length'].fillna(0)).describe())


def convert_to_span_format(numbers):
    sorted_numbers = sorted(list(numbers))
    spans = []
    if sorted_numbers:
        start = sorted_numbers[0]
        end = sorted_numbers[0]
        for num in sorted_numbers[1:]:
            if num == end + 1:
                end = num
            else:
                spans.append((start, end))
                start = end = num
        spans.append((start, end))
    return ' '.join(f'<<{start}-{end}>>' for start, end in spans)

def compute_span_differences(dataframe):
    differences = {}
    for question, group in dataframe.groupby("Question"):
        settings = group["Setting"].unique()
        if len(settings) > 1:
            for i in range(len(settings)):
                for j in range(i+1, len(settings)):
                    setting_1 = settings[i]
                    setting_2 = settings[j]
                    room_1 = group[group["Setting"] == setting_1]["Room name"].values[0]
                    room_2 = group[group["Setting"] == setting_2]["Room name"].values[0]
                    acc_1 = group[group["Setting"] == setting_1]["Final_Accuracy"].values[0]
                    acc_2 = group[group["Setting"] == setting_2]["Final_Accuracy"].values[0]
                    span_str_1 = group[group["Setting"] == setting_1]["merged_quote_spans"].values[0]
                    span_str_2 = group[group["Setting"] == setting_2]["merged_quote_spans"].values[0]
                    numbers_1 = extract_numbers_from_span(span_str_1)
                    numbers_2 = extract_numbers_from_span(span_str_2)
                    diff_1 = numbers_1 - numbers_2
                    diff_2 = numbers_2 - numbers_1
                    key = (question, setting_1, room_1, acc_1, setting_2, room_2, acc_2)
                    value = (convert_to_span_format(diff_1), convert_to_span_format(diff_2))
                    differences[key] = value
    return differences

span_differences_all = compute_span_differences(debater_turns_agg)

#print(span_differences_all.keys())
#for span in span_differences_all[('Why were Jorgenson and Ganti not put to death?', 'Human Consultancy Dishonest', 'Human Consultancy Honest')]:
#  print(len(quote_length(span)))
```

```{python quote span difference}
split_span_differences_with_room = []
# Iterate over the span differences
for (question, setting_1, room_1, acc_1, setting_2, room_2, acc_2), (diff_1, diff_2) in span_differences_all.items():
    split_span_differences_with_room.append((question, setting_1, room_1, acc_1, setting_2, room_2, acc_2, diff_1))
    split_span_differences_with_room.append((question, setting_2, room_2, acc_2, setting_1, room_1, acc_1, diff_2))
    
# Convert the list to a DataFrame
split_span_df = pd.DataFrame(split_span_differences_with_room, columns=['Question', 'Setting 1', 'Room 1', 'Acc_1', 'Setting 2', 'Room 2', 'Acc_2', 'Span Difference'])

split_span_df["Span Difference Count"] = split_span_df["Span Difference"].apply(lambda x: len(quote_length(x)))
split_span_df["Settings"] = split_span_df["Setting 1"] + " - " + split_span_df["Setting 2"]


# Group by the new 'Settings' column and compute aggregated counts and average of 'Span Difference Count'
grouped_data = split_span_df.groupby("Settings").agg(
    Count=('Span Difference Count', 'size'),
    Average_Span_Difference=('Span Difference Count', 'mean')
).reset_index()

grouped_data

filtered_df = split_span_df[
    (split_span_df["Setting 1"] == "Human Debate") &
    ((split_span_df["Setting 2"] == "Human Consultancy Honest") | (split_span_df["Setting 2"] == "Human Consultancy Dishonest"))
]

print(filtered_df.groupby(['Setting 2','Acc_1','Acc_2'])['Span Difference Count'].describe())

# Calculate the IQR and bounds for each group in 'Setting 2'
grouped = filtered_df.groupby('Setting 2')['Span Difference Count']

Q1 = grouped.quantile(0.25)
Q3 = grouped.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out the outliers based on the computed bounds
filtered_no_outliers = filtered_df[
    (filtered_df['Setting 2'].map(lower_bound) <= filtered_df['Span Difference Count']) &
    (filtered_df['Setting 2'].map(upper_bound) >= filtered_df['Span Difference Count'])
]

filtered_no_outliers

print(filtered_no_outliers.groupby(['Setting 2','Acc_1','Acc_2'])['Span Difference Count'].describe())

```

```{r quote_length graph, out.width = "100%", dpi = 300}
debater_turns<- py$debater_turns_agg_simple
debater_turns$check <- paste0(debater_turns$Participant_y, debater_turns$`Room name`)
sample.rooms <- read.csv("~/Downloads/sample-rooms-2.csv", header=FALSE)
sample.rooms_samples <- sort(paste0(sample.rooms$V2, sample.rooms$V1))
debater_turns <- subset(debater_turns, debater_turns$check %in% sample.rooms_samples)

span_difference_debate_consultancies<-py$filtered_df
ggplot(span_difference_debate_consultancies) +
  geom_boxplot(aes(x = `Setting 2`, y = `Span Difference Count`))

final_table_desc_stats <- debater_turns %>% group_by(Final_Setting) %>% summarise(n = n(), rounds = mean(`Number of judge continues`), quotes = mean(quote_length), quotes.rounds = mean(quote_length/`Number of judge continues`))
knitr::kable(final_table_desc_stats, booktab = TRUE, digits = 1, col.names = c("Setting", "n", "rounds per debate", "quoted tokens per debate", "tokens per round"))

filtered_outliers <- debater_turns %>%
  group_by(Final_Setting) %>%
  mutate(Q1 = quantile(quote_length, 0.25),
         Q3 = quantile(quote_length, 0.75),
         IQR = Q3 - Q1,
         lower_bound = Q1 - 1.5 * IQR,
         upper_bound = Q3 + 1.5 * IQR)

ggplot(data = debater_turns) +
  geom_histogram(aes(x = quote_length, binwidth = 1)) +
  facet_wrap(~ Final_Setting)

pairwise.t.test(debater_turns$quote_length, debater_turns$Final_Setting)


ggplot(debater_turns) +
  geom_boxplot(aes(x = Final_Setting, y = `Quote length`)) +
  labs(y = "Total Quote Length (debater_turns)")+
  theme_minimal()
filtered <- debater_turns %>%
  group_by(Final_Setting) %>%
  mutate(Q1 = quantile(quote_length, 0.25),
         Q3 = quantile(quote_length, 0.75),
         IQR = Q3 - Q1,
         lower_bound = Q1 - 1.5 * IQR,
         upper_bound = Q3 + 1.5 * IQR) %>%
  filter(quote_length > 0 & quote_length < 750) %>%
  select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound) 
filtered %>%
  ggplot() +
  geom_boxplot(aes(x = Final_Setting, y = quote_length)) +
  labs(y = "Total Quote Length in a Debate/Consultancy (unique tokens)", x = "Setting")+
  theme_minimal()
debater_turns %>%
  ggplot() +
  geom_violin(aes(x = Final_Setting, y = quote_length)) +
  geom_dotplot(aes(x = Final_Setting, y = quote_length), binaxis= "y",
               stackdir = "center",
               dotsize = 0.5,
               fill = 1) +
  labs(y = "Total Quote Length in a Debate/Consultancy (unique tokens)", x = "Setting")+
  theme_minimal()


debater_turns %>%
  group_by(Final_Setting) %>%
  summarise(avg = mean(quote_length),
            lower_ci = t.test(quote_length)$conf.int[1],
            upper_ci = t.test(quote_length)$conf.int[2]) %>%
  ggplot(aes(x = avg)) +
  geom_histogram(data = debater_turns, aes(x = quote_length), binwidth = 100, alpha = 0.25) +
  geom_vline(aes(xintercept = avg, color = Final_Setting), linetype="dashed", size=1) +
  geom_rect(aes(xmin = lower_ci, xmax = upper_ci, ymin = -Inf, ymax = Inf, fill = Final_Setting), alpha = 0.25) +
  labs(x = "Total Quote Length in a Debate/Consultancy (unique tokens) per Setting", 
       y = "Frequency") +
  facet_wrap(~Final_Setting, ncol = 1, strip.position = "left") +
  theme_minimal() +
  theme(
    axis.title.y.right = element_text(angle = 90),
  ) + 
  scale_y_continuous(position = "right") +
  theme(legend.position="none")



pairwise.t.test(filtered$quote_length, filtered$Final_Setting)


filtered %>% group_by(Final_Setting) %>% summarise(avground = median(quote_length))
debater_turns %>% group_by(Final_Setting) %>% summarise(avground = median(quote_length))
```

```{r rounds graph, out.width = "100%", dpi = 300}
debater_turns <- debater_turns %>%
  group_by(`Room name`) %>%
  mutate(`Max judge rounds by room` = max(`Number of judge continues`, na.rm = TRUE)) %>%
  ungroup()
debater_turns <- debater_turns %>%
  mutate(`Max judge rounds bin` = factor(ifelse(`Max judge rounds by room` > 7, "8", as.character(`Max judge rounds by room`))))

table(debater_turns$`Max judge rounds bin`)
ggplot(debater_turns) +
  geom_boxplot(aes(x = Final_Setting, y = `Max judge rounds by room`)) +
  labs(y = 'Max Judging Rounds') +
  theme_minimal() 

pairwise.t.test(debater_turns$`Max judge rounds by room`, debater_turns$Final_Setting)

table(round(debater_turns$quote_length, -2))
debater_turns$quote_length_bin <- as.factor(round(debater_turns$quote_length, -2))
debater_turns$quote_length_bin <- ordered(debater_turns$quote_length_bin, levels = paste(sort(as.integer(levels(debater_turns$quote_length_bin)))))
table(debater_turns$quote_length_bin)

# Define the color function and palette
colfunc <- colorRampPalette(c(correctColor,"white",incorrectColor))
palette <- colfunc(length(levels(as.factor(debater_turns$`Max judge rounds bin`))))

# Plot
debater_turns %>%
  filter(`Max judge rounds bin` != "0") %>%
  group_by(Final_Setting) %>%
  summarise(avg = mean(as.numeric(levels(quote_length_bin))[quote_length_bin], na.rm = T),
            lower_ci = t.test(as.numeric(levels(quote_length_bin))[quote_length_bin], na.rm = T)$conf.int[1],
            upper_ci = t.test(as.numeric(levels(quote_length_bin))[quote_length_bin], na.rm = T)$conf.int[2],
            n = n())
debater_turns %>%
  filter(`Max judge rounds bin` != "0" & !is.na(Final_Setting)) %>%
  group_by(Final_Setting) %>%
  summarise(avg = mean(as.numeric(levels(quote_length_bin))[quote_length_bin], na.rm = T),
            lower_ci = t.test(as.numeric(levels(quote_length_bin))[quote_length_bin], na.rm = T)$conf.int[1],
            upper_ci = t.test(as.numeric(levels(quote_length_bin))[quote_length_bin], na.rm = T)$conf.int[2]) %>%
  ggplot(aes(x = avg)) +
  geom_bar(data = debater_turns %>% filter(`Max judge rounds bin` != "0" & !is.na(Final_Setting)), 
                 aes(x = as.numeric(levels(quote_length_bin))[quote_length_bin], fill = as.factor(`Max judge rounds bin`)), 
                 position='stack', 
                 color = "black",
                 size = 0.1) +
  #geom_vline(aes(xintercept = avg), size=1, color = "black") +
  #geom_rect(aes(xmin = lower_ci, xmax = upper_ci, ymin = -Inf, ymax = Inf), alpha = 0.25, fill = "black") +
  labs(x = "Total Unique Quote Tokens", 
       y = "Frequency") +
  facet_wrap(~Final_Setting, ncol = 1, strip.position = "left") +
  scale_fill_manual(values = palette, name = "Total Rounds") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_bw(base_size = 16) +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(linewidth = 1),
        panel.grid.minor.y = element_line(linewidth = 0.25))



colfunc <- colorRampPalette(c(correctColor,"white",incorrectColor))
palette <- colfunc(length(levels(as.factor(debater_turns$quote_length_bin))))
debater_turns %>%
  filter(`Max judge rounds by room` != "0" & !is.na(Final_Setting)) %>%
  group_by(Final_Setting) %>%
  summarise(avg = mean(`Max judge rounds by room`),
            lower_ci = t.test(`Max judge rounds by room`)$conf.int[1],
            upper_ci = t.test(`Max judge rounds by room`)$conf.int[2]) %>%
  ggplot(aes(x = avg)) +
  geom_histogram(data = debater_turns %>% filter(`Max judge rounds by room` != "0" & !is.na(Final_Setting)), 
                 aes(x = `Max judge rounds by room`, fill = quote_length_bin), 
                 position='stack', 
                 binwidth = 1,
                 color = "black",
                 size = 0.1) +
  geom_vline(aes(xintercept = avg), size=1, color = "black") +
  geom_rect(aes(xmin = lower_ci, xmax = upper_ci, ymin = -Inf, ymax = Inf), alpha = 0.25, fill = "black") +
  labs(x = "Total Rounds", 
       y = "Frequency") +
  facet_wrap(~Final_Setting, ncol = 1, strip.position = "left", scales = "free_y") +
  scale_fill_manual(values = palette, name = "Total\nUnique\nTokens*") +
  scale_x_continuous(breaks = 1:25, expand = expansion(mult = c(0, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_bw(base_size = 16) +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(linewidth = 1),
        panel.grid.minor.y = element_line(linewidth = 0.25))




debater_turns %>%
  filter(`Max judge rounds by room` != "0" & !is.na(Final_Setting)) %>%
  group_by(`Max judge rounds by room`, quote_length, Final_Setting) %>%
  mutate(counts = n()) %>%
  ggplot() +
  geom_point(aes(x = `Max judge rounds by room`, 
                 y = quote_length,  
                 color = Final_Setting,
                 #fill = Final_Setting,
                 size = counts,
                 stroke = 1),
             alpha = 0.65,
             shape = 21) +
  geom_smooth(aes(x = `Max judge rounds by room`, 
                  y = quote_length, 
                  color = Final_Setting,
                  fill = Final_Setting),  # Added fill aesthetic here
              method = "lm", 
              linetype = "solid") +
  labs(x = "Total Rounds", 
       y = "Total Quote Tokens*",
       color = "Settings:") +
  guides(size = "none", fill = "none", 
         color = guide_legend(override.aes = list(fill = "white"))) +
  scale_x_continuous(breaks = 0:25) +
  scale_color_manual(values = c("#32759b", "#a3d6d2", "#d14904", "#fdc998")) +
  scale_fill_manual(values = c("#32759b", "#a3d6d2", "#d14904", "#fdc998")) +  # Set fill colors here
  theme_bw(base_size = 16) +
  theme(legend.position = "top")


ggsave("efficiency_rounds_tokens.png", plot = last_plot(), width = 13, height = 8, bg = "white", dpi = 300)

debater_turns %>%
  filter(`Max judge rounds by room` != "0" & !is.na(Final_Setting)) %>%
  group_by(Final_Setting) %>%
  summarise(avg = mean(`Max judge rounds by room`),
            lower_ci = t.test(`Max judge rounds by room`)$conf.int[1],
            upper_ci = t.test(`Max judge rounds by room`)$conf.int[2])
debater_turns %>%
  filter(`Max judge rounds by room` & !is.na(Final_Setting)) %>%
  group_by(Final_Setting) %>%
  summarise(avg = mean(quote_length),
            lower_ci = t.test(quote_length)$conf.int[1],
            upper_ci = t.test(quote_length)$conf.int[2])
```

### Length of debates, stratified

```{python strat, out.width = "100%", dpi = 300}
all_turns = turns.merge(
        judgments_online[["Room name", "Honest debater", "Dishonest debater", "Question", "Article ID",
                 "Speed annotator accuracy","Untimed annotator context","Untimed annotator context bins","Is offline","Final_Setting", "Setting","Number of judge continues"]],
        how="left",
        on="Room name",
    )
```


```{r strat ggplot, width='100%', dpi=300}
strat <- py$all_turns
strat <- subset(strat, strat$Role == "Judge")
strat <- strat %>%
  group_by(`Room name`, Participant) %>%
  mutate(`Max judge rounds` = max(`Number of judge continues`, na.rm = TRUE)) %>%
  ungroup()

# Bootstrap mean function
bootstrap_mean <- function(data, indices) {
  return(mean(data[indices], na.rm = TRUE))
}


# Extract unique bin values
unique_bins <- levels(strat$`Max judge rounds`)[2:length(levels(strat$`Max judge rounds`))]

# Create a decreasing sequence of alpha values
alpha_values <- seq(1, 0.1, length.out = length(unique_bins))

# Create a named vector for mapping
alpha_map <- setNames(alpha_values, unique_bins)

strat %>%
  filter(`Max judge rounds` != "0"& !is.na(Final_Setting)) %>% 
  group_by(Final_Setting, `Number of judge continues`, `Max judge rounds`) %>%
  do({
    boot_result <- boot(data = .$`Probability correct`, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025, na.rm = TRUE),
      upper_ci = quantile(boot_result$t, 0.975, na.rm = TRUE)
    )
  }) %>%
  ggplot(aes(x = `Number of judge continues`, y = mean_accuracy, col = as.factor(`Max judge rounds`))) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = as.factor(`Max judge rounds`), group = as.factor(`Max judge rounds`), color = NULL), alpha = 0.25) +
  labs(title = "Average Probability Correct by Round, \nStratified by binned Max Round",
       x = "Round", 
       y = "Average Intermediate Probability Correct") +
  geom_line() +
  #scale_alpha_manual(values = alpha_map) +
  facet_wrap(~Final_Setting) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(alpha = "none")



strat <- strat %>%
  mutate(
    `Max judge rounds bin` = case_when(
      `Max judge rounds` <= 0 ~ "0",
      `Max judge rounds` <= 2 ~ "1-2",
      `Max judge rounds` <= 4 ~ "3-4",
      `Max judge rounds` <= 6 ~ "5-6",
      `Max judge rounds` <= 8 ~ "7-8",
      TRUE ~ "9+"
    )
  ) %>%
  mutate(
    `Max judge rounds bin` = factor(
      `Max judge rounds bin`,
      levels = rev(c("0","1-2","3-4","5-6", "7-8","9+")),
      ordered = TRUE
    )
  )

table(strat$`Max judge rounds`)
table(strat$`Max judge rounds bin`)
strat %>%
  filter(`Max judge rounds` != "0" & !is.na(Final_Setting)) %>%  # Remove entries with "0" bin
  group_by(Final_Setting, `Number of judge continues`, `Max judge rounds`) %>%
  do({
    boot_result <- boot(data = .$`Probability correct`, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025, na.rm = TRUE),
      upper_ci = quantile(boot_result$t, 0.975, na.rm = TRUE)
    )
  }) %>%
  ggplot(aes(x = `Number of judge continues`, y = mean_accuracy, col = as.factor(`Max judge rounds`))) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = as.factor(`Max judge rounds`), color = NULL), alpha = 0.2) +
  labs(x = "Rounds", 
       y = "Average Intermediate Probability Correct",
       col = "Settings") +  # Rename the legend
  geom_line() +
  facet_wrap(~Final_Setting) +
  guides(alpha = "none", color = "none", fill = guide_legend(nrow = 1)) +  # Specify that legend should be displayed in a single row
  theme_bw(base_size = 16) +
  theme(legend.position = "top")  # Specify legend position



strat$reward_unpenalized <- log2(strat$`Probability correct`)
strat %>%
  filter(`Max judge rounds` != "0" & !is.na(Final_Setting)) %>%  # Remove entries with "0" bin
  group_by(Final_Setting, `Number of judge continues`, `Max judge rounds bin`) %>%
  do({
    boot_result <- boot(data = .$reward_unpenalized, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025, na.rm = TRUE),
      upper_ci = quantile(boot_result$t, 0.975, na.rm = TRUE)
    )
  }) %>%
  ggplot(aes(x = `Number of judge continues`, y = mean_accuracy, col = `Max judge rounds bin`)) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = `Max judge rounds bin`, color = NULL), alpha = 0.2) +
  labs(x = "Rounds", 
       y = "Average Reward (unpenalized)",
       col = "Settings") +  # Rename the legend
  geom_line() +
  facet_wrap(~Final_Setting) +
  guides(alpha = "none", color = "none", fill = guide_legend(nrow = 1)) +  # Specify that legend should be displayed in a single row
  theme_bw(base_size = 16) +
  theme(legend.position = "top")  # Specify legend position

colfunc <- colorRampPalette(c("grey", "black"), bias = 3)
palette <- colfunc(length(c("0-1", "2-3", "4-5", "6-7")))

strat %>%
  filter(`Max judge rounds` != "0" & !is.na(Final_Setting) &
        `Max judge rounds bin` %in% c("1-2","3-4","5-6", "7-8")) %>%  # Remove entries with "9+" bin
  group_by(Final_Setting, `Number of judge continues`, `Max judge rounds bin`) %>%
  do({
    boot_result <- boot(data = .$reward_unpenalized, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025, na.rm = TRUE),
      upper_ci = quantile(boot_result$t, 0.975, na.rm = TRUE)
    )
  }) %>%
  ggplot(aes(x = `Number of judge continues`, y = mean_accuracy, col = `Max judge rounds bin`)) +
  #geom_hline(yintercept = -1, linetype="solid", color = "black") +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = `Max judge rounds bin`, color = NULL), alpha = 0.25) +
  labs(x = "Rounds", 
       y = "Average Reward (unpenalized)",
       col = "Settings") +  # Rename the legend
  geom_line(linewidth=1.5) +
  facet_wrap(~Final_Setting) +
  #scale_color_brewer(palette = "Set1") +
  #scale_fill_brewer(palette = "Set1") +
  scale_color_manual(values = c("#fdc998", "#a3d6d2","#32759b", "#d14904")) +
  scale_fill_manual(values = c( "#fdc998", "#a3d6d2", "#32759b","#d14904")) +
  guides(alpha = "none", fill = "none", color = guide_legend(nrow = 1, title = "Max Judge Rounds (binned)", override.aes = list(fill = "white"))) +  # Specify that legend should be displayed in a single row
  theme_bw(base_size = 24) +
  theme(legend.position = "top") +
  coord_cartesian(ylim = c(-2, NA))



ggsave("main_info_accuracy.png", plot = last_plot(), width = 13, height = 8, bg = "white", dpi = 300)


# Split strat into subsets based on Max judge rounds
strat_split <- split(strat, strat$`Max judge rounds bin`)

# Define the analysis function
analysis_function <- function(df) {
  df %>%
    filter(`Max judge rounds` != "0" & !is.na(Final_Setting)) %>%
    group_by(Final_Setting, `Number of judge continues`) %>%
    summarise(mean_prob_correct = mean(log2(`Probability correct`), na.rm = TRUE)) %>%
    mutate(diff = mean_prob_correct - lag(mean_prob_correct)) %>%
    summarise(mean_diff = mean(diff, na.rm=TRUE))
}

# Apply the analysis function to each subset
results <- map(strat_split, analysis_function)

# Get the unique values of 'Max judge rounds' (assuming they are in the same order as 'results')
max_judge_rounds_values <- as.numeric(names(results))

# Add a column to each data frame in 'results' to identify the value of 'Max judge rounds'
results <- map2(results, max_judge_rounds_values, ~ mutate(.x, `Max judge rounds` = .y))

# Combine the list of data frames into a single data frame
final_result <- bind_rows(results)

```

### Time (offline judging..?)

```{python TODO offline judging}
# Convert to datetime
judgments["Offline judging start time"] = pd.to_datetime(judgments["Offline judging start time"], unit="ms")
judgments["Offline judging end time"] = pd.to_datetime(judgments["Offline judging end time"], unit="ms")

# Calculate offline judging time in minutes
judgments["Offline judging time"] = (judgments["Offline judging end time"] - judgments["Offline judging start time"]).dt.total_seconds() / 60


print(f"Number of offline judgments on consultancies:\n{judgments[judgments['Setting'].str.contains('Consultancy')]['Offline judging time'].dropna().describe()}\nOnly 13...")

# Filter out rows with NaT values
valid_judging_time = judgments["Offline judging time"].dropna()

# Calculate summary statistics
summary_stats = valid_judging_time.describe()
print(summary_stats)


# Filter judgments with offline judging time above 65 minutes
filtered_judgments = judgments[(judgments["Offline judging time"] < 65) & (judgments["Untimed annotator context"] > 0)]

# Print filtered judgments
# print("Filtered judgments with offline judging time above 65 minutes:")
print(filtered_judgments['Offline judging time'].describe())

# Create the histogram
plt.hist(filtered_judgments['Offline judging time'], bins=10)

# Set labels and title
plt.xlabel("Offline Judging Time (minutes)")
plt.ylabel("Frequency")
plt.title("Histogram of Offline Judging Time")

# Display the histogram
plt.show()

aggregates = {
    'Final probability correct': 'mean',
    'Untimed annotator context': 'mean'
}
filtered_judgments = filtered_judgments.groupby('Offline judging time').agg(aggregates).reset_index()


```

# Analysis

## Question Difficulty

confounder rounds, quotes
```{python check qdiff/rounds/setting}
judgments["Number of judge continues bins"] = pd.cut(
    judgments["Number of judge continues"], 
    bins=[0, 3, 6, 9, float('inf')],  # bin edges
    labels=['1-3', '4-6', '7-9', '10+'],  # labels for the resulting bins
    right=True  # includes the right edge of the bin
)
aggregated_df = judgments.groupby(["Setting", "Number of judge continues bins"])["Final_Accuracy"].agg(
    Proportion_True=lambda x: x.mean(),
    Total_Count="size"
).reset_index()
pd.set_option('display.max_columns', None)
print(aggregated_df)
pd.reset_option('display.max_columns')

total_counts_for_setting = judgments.groupby('Final_Setting').size()
result = judgments.groupby(["Final_Setting", "Untimed annotator context bins", "Number of judge continues bins"]).agg(
    Proportion_True=pd.NamedAgg(column='Final_Accuracy', aggfunc=lambda x: x.mean()),
    Context_Count=pd.NamedAgg(column='Final_Accuracy', aggfunc='size'),
    Proportion_Context=pd.NamedAgg(column='Final_Setting', aggfunc=lambda x: len(x) / total_counts_for_setting[x.mode()])
).reset_index()
print(f'Is it number of rounds (meaning more evidence) that confounds the consultancy accuracy?:\n{result}')
```

```{r Accuracy by Context Graph, out.width = "100%", dpi = 300}
judgments$`Untimed annotator context bins` <- as.factor(judgments$`Untimed annotator context bins`)

bootstrap_mean <- function(data, indices) {
  return(mean(data[indices], na.rm = TRUE))
}

judgments_online %>%
  group_by(`Untimed annotator context bins`, Final_Setting) %>%
  do({
    boot_result <- boot(data = .$Final_Accuracy, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025),
      upper_ci = quantile(boot_result$t, 0.975)
    )
  }) %>%
  ggplot(aes(x = `Untimed annotator context bins`, y = mean_accuracy, color = Final_Setting, group = Final_Setting)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = Final_Setting, color = NULL), alpha = 0.25) +
  labs(y = "Average Final Accuracy", x = "Untimed Annotator Context") +
  theme_minimal() +
  facet_wrap(~ Final_Setting)
```

## Judge Skill

### Judge "Experience"

```{r, out.width = "100%", dpi = 300}
test_trend <- judgments_online %>%
  arrange(Final_Setting, Participant, `End time`) %>%
  group_by(Final_Setting, `End time`) %>%
  summarise(Final_Accuracy=as.numeric(Final_Accuracy)) %>%
  spread(key = Final_Setting, value = Final_Accuracy) %>%
  select(-`End time`)

test_trend <- judgments_online %>%
  arrange(Final_Setting, Participant, `End time`) %>%
  group_by(Final_Setting, `End time`) %>%
  summarise(Final_Accuracy=as.numeric(Final_Accuracy)) %>%
  spread(key = Final_Setting, value = Final_Accuracy) %>%
  select(-`End time`)

library(funtimes)
apply(test_trend, 2, function(x) notrend_test(na.omit(x))$p.value)

judgments_online %>% 
  group_by(Final_Setting, Participant) %>%
  arrange(`End time`) %>%
  mutate(count=row_number()) %>% 
  group_by(Final_Setting, count) %>%
  do({
    boot_result <- boot(data = .$Final_Accuracy, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025, na.rm = TRUE),
      upper_ci = quantile(boot_result$t, 0.975, na.rm = TRUE)
    )
  }) %>%
  ggplot(aes(x = count, y = mean_accuracy, color = Final_Setting, group = Final_Setting)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = Final_Setting, color = NULL), alpha = 0.25) +
  labs(y = "Average Final Accuracy", x = "Judging Counts") +
  theme_minimal() +
  facet_wrap(~ Final_Setting)

subset(judgments_online, judgments_online['Setting'] == 'Human Debate') %>% 
  group_by(`Untimed annotator context bins`, Participant) %>%
  arrange(`End time`) %>%
  mutate(count=row_number()) %>% 
  group_by(`Untimed annotator context bins`, count) %>%
  do({
    boot_result <- boot(data = .$Final_Accuracy, statistic = bootstrap_mean, R = 1000)
    data.frame(
      mean_accuracy = mean(boot_result$t, na.rm = TRUE),
      lower_ci = quantile(boot_result$t, 0.025, na.rm = TRUE),
      upper_ci = quantile(boot_result$t, 0.975, na.rm = TRUE)
    )
  }) %>%
  ggplot(aes(x = count, y = mean_accuracy, color = `Untimed annotator context bins`, group = `Untimed annotator context bins`)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = `Untimed annotator context bins`, color = NULL), alpha = 0.25) +
  labs(y = "Average Final Accuracy", x = "Judging Counts") +
  theme_minimal() +
  facet_wrap(~ `Untimed annotator context bins`)
```

### Calibration

S: (1) debaters didnt learn calibration -\> calibration over time? S: (2) dishonest debater tricks
```{r confident mistakes}
library(ggplot2)
library(dplyr)

# Segregate confidently correct and confidently wrong
judgments_online$confidence_label <- case_when(
  judgments_online$`Final probability correct` > 0.95 ~ "Confidently Correct",
  judgments_online$`Final probability correct` < 0.05 ~ "Confidently Wrong",
  TRUE ~ "Neutral"
)

# Filter out only the rows with confidently correct and confidently wrong labels
filtered_data <- judgments_online %>%
  filter(confidence_label != "Neutral")

# Count the occurrences for each setting and confidence label
count_data <- filtered_data %>%
  group_by(`Final_Setting`, confidence_label) %>%
  summarise(count = n())

# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = confidence_label)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("Confidently Correct" = correctColor, "Confidently Wrong" = incorrectColor)) +
  labs(title = "Confident Mistakes and Correct by Setting", y = "Count", x = "Setting") +
  theme_minimal()





# Calculate the color value for each row
judgments_online$color_value <- log2(judgments_online$`Final probability correct`) - (0.05 * judgments_online$`Number of judge continues`)

# Count the occurrences for each setting and 'Final probability correct' value
count_data <- judgments_online %>%
  group_by(`Final_Setting`, `Final probability correct`, color_value) %>%
  summarise(count = n())

# Plot
ggplot(count_data, aes(x = `Final_Setting`, y = count, fill = color_value, group = `Final probability correct`)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_gradient(low = "#DC143C", high = "#008000") +  # Adjust as needed
  labs(title = "Distribution of Final Probabilities by Setting", y = "Count", x = "Setting") +
  theme_minimal()

```


```{python calibration, out.width = "100%", dpi = 300}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve

def calibration_plot(df, setting_name, ax=None):
    df['outcome'] = pd.Series(df['Final probability correct'] > 0.5, dtype=int)
    df['confidence'] = df['Final probability correct'].apply(lambda x: x if x > 0.5 else 1 - x)
    df['bins'] = pd.cut(df['confidence'], [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99])
    # Group by bins and calculate the mean outcome
    df_grouped = df.groupby('bins')['outcome'].mean().reset_index()
    # Compute standard error in each bin
    std_error = df.groupby('bins')['outcome'].apply(lambda x: x.std() / np.sqrt(len(x)) if len(x) > 1 else 0)
    df_grouped['std_error'] = df['bins'].cat.categories.map(std_error)
    if ax is None:
        plt.rcParams.update({'font.size': 16})
        fig, ax = plt.subplots(figsize=(8, 6))
    # Plot the calibration curve with error bars
    ax.plot(df_grouped['bins'].apply(lambda x: x.mid), df_grouped['outcome'], marker='o', linewidth=2, label='Calibration Curve')
    ax.errorbar(df_grouped['bins'].apply(lambda x: x.mid), df_grouped['outcome'], yerr=df_grouped['std_error'], fmt='o', capsize=5, linewidth=2, label='Error Bars')
    ax.set_xlabel('Final judge probability')
    ax.set_ylabel('Accuracy')
    ax.set_title(f'Judge calibration for {setting_name}')
    ax.plot([0.5, 1], [0.5, 1], linestyle='--', color='gray', label='Perfect Calibration')
    ax.grid(True)
    ax.legend()
    # Calculate ECE
    actual_labels = df['outcome'].values
    predicted_probs = df['Final probability correct'].values
    prob_true, prob_pred = calibration_curve(actual_labels, predicted_probs, n_bins=10)
    ece = np.mean(np.abs(prob_pred - prob_true) * (prob_true.size / len(actual_labels)))
    # Print ECE
    print(f"Expected Calibration Error (ECE) for {setting_name}: {ece:.4f}")
    plt.show()
    plt.rcParams.update({'font.size': plt.rcParamsDefault['font.size']})

# Loop through each unique setting and create a calibration plot
for setting in judgments_online['Final_Setting'].unique():
    setting_df = judgments_online[judgments['Final_Setting'] == setting].copy()
    calibration_plot(setting_df, setting)

```

### Judge Involvement

### Judge Mistakes

## Debater Skill
```{python leaderboard}
leaderboard = sessions.merge(
        debates[
            [
                "Room name",
                "Honest debater",
                "Dishonest debater"
            ]
        ],
        how="left",
        on="Room name",
    )



leaderboard = sessions.merge(
        debates[
            [
                "Room name",
                "Honest debater",
                "Dishonest debater"
            ]
        ],
        how="left",
        on="Room name",
    )
# filter to sessions corresponding to a judge
leaderboard = leaderboard[leaderboard['Final probability correct'].notna()]
from math import log
leaderboard['Judge reward'] = leaderboard.apply(
    lambda row: log(row['Final probability correct']) - (0.05 * row['Number of judge continues']),
    axis=1
)
leaderboard['Honest debater reward'] = leaderboard.apply(
    lambda row: log(row['Final probability correct']),
    axis=1
)
leaderboard['Dishonest debater reward'] = leaderboard.apply(
    lambda row: log(1 - row['Final probability correct']),
    axis=1
)
```

```{r leaderboard rank}
leaderboard<-py$leaderboard
leaderboard <- leaderboard %>%
  group_by(Participant) %>%
  mutate(Avg_Judge_Reward = mean(`Judge reward`, na.rm = TRUE)) %>%
  ungroup()
leaderboard <- leaderboard %>%
  group_by(`Honest debater`) %>%
  mutate(Avg_Honest_Debater_Reward = mean(`Honest debater reward`, na.rm = TRUE)) %>%
  ungroup()
leaderboard <- leaderboard %>%
  group_by(`Dishonest debater`) %>%
  mutate(Avg_Dishonest_Debater_Reward = mean(`Dishonest debater reward`, na.rm = TRUE)) %>%
  ungroup()
leaderboard <- leaderboard %>%
  mutate(Rank_Judge = rank(desc(Avg_Judge_Reward)),
         Rank_Honest = rank(desc(Avg_Honest_Debater_Reward)),
         Rank_Dishonest = rank(desc(Avg_Dishonest_Debater_Reward)))
```

```{r deprecated}
random.intercept.model = lmer(`Final probability correct` ~  (1|Final_Setting), 
                              data = judgments, REML = TRUE)

judgments$random.intercept.preds = predict(random.intercept.model)

colnames(judgments)
dishonest <- judgments[!is.na(judgments$`Dishonest debater`), ]
model3 <- glm(Final_Accuracy ~ relevel(factor(`Dishonest debater`), 'Shlomo Kofman') + relevel(factor(Final_Setting), 'Human Debate'), family = 'binomial', data = judgments[!is.na(judgments$`Dishonest debater`), ])
summary(model3)

result <- judgments_online %>%
  group_by(`Dishonest debater`) %>%
  summarize(
    Win_Rate = sum(Final_Accuracy == "FALSE") / n()
  ) %>%
  ungroup() %>%
  arrange(desc(Win_Rate))

result

result1 <- judgments_online %>%
  group_by(`Honest debater`) %>%
  summarize(
    Win_Rate = sum(Final_Accuracy == "TRUE") / n()
  ) %>%
  ungroup() %>%
  arrange(desc(Win_Rate))

result1



# Filter for high win rate debaters
high_win_rate_debaters <- result1 %>%
  filter(Win_Rate > 0.90)  # Set the threshold for high win rate

# Filter original data for debates with 'Debate' in Final_Setting
filtered_data <- judgments_online %>%
  filter(grepl("Debate", Final_Setting)) 

# Find cases where high win rate debaters lost
cases_high_win_rate_lost <- filtered_data %>%
  filter(`Honest debater` %in% high_win_rate_debaters$`Honest debater` & Final_Accuracy != "TRUE")

cases_high_win_rate_lost




# Filter for high win rate debaters
high_win_rate_debaters <- result %>%
  filter(Win_Rate > 0.20)  # Set the threshold for high win rate

# Filter original data for debates with 'Debate' in Final_Setting
filtered_data <- judgments_online %>%
  filter(grepl("Debate", Final_Setting)) 

# Find cases where high win rate debaters lost
cases_high_win_rate_lost <- filtered_data %>%
  filter(`Dishonest debater` %in% high_win_rate_debaters$`Dishonest debater` & Final_Accuracy != "FALSE")

cases_high_win_rate_lost


# Fit the random intercept model and only remove missing values for 'Dishonest debater'
random_intercept_model <- lmer(`Final probability correct` ~ (1|`Dishonest debater`), 
                                data = dishonest, 
                                REML = TRUE)

# Summary of the model
summary(random_intercept_model)
dishonest$random.intercept.preds = predict(random_intercept_model)
plot(dishonest$random.intercept.preds, dishonest$`Final probability correct`)

```

### Debater "Experience", ratings - how many wins?

### AI vs Humans

### Old vs New






### possibly unnessary
Finally, these are how many we get correct in each setting

```{r quick ori acc stats, out.width = "100%", dpi = 300}
judgments_online <- py$judgments_online
table(judgments_online$Final_Accuracy, judgments_online$Final_Setting)
table(judgments_online$Final_Accuracy, judgments_online$Setting)

ggplot(judgments_online, aes(x = Final_Setting, fill = Final_Accuracy)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("TRUE" = "green", "FALSE" = "red")) +
  labs(title = "Judgments by Setting, overall", x = "Setting", y = "Proportion", fill = "Final_Accuracy") +
  theme_minimal() +
  theme(axis.text.x = element_text())
```

Sneak peak of accuracy differences between judges, but we won't get to that again until models

```{r quick ori stats cont, out.width = "100%", dpi = 300}
ggplot(judgments_online, aes(x = Final_Setting, fill = Final_Accuracy)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("TRUE" = "green", "FALSE" = "red")) +
  labs(title = "Judgments by Setting, per judge", x = "Setting", y = "Proportion", fill = "Final_Accuracy") +
  theme_minimal() +
  theme(axis.text.x = element_text(),#angle = 90, hjust = 1),
        axis.text.y = element_blank(),
        strip.text.y.right = element_text(angle = 0)) +
  facet_grid(rows = "Participant")
```


